# Pre-workshop guide

<br><br>

## How to make the most of this workshop

-   Work on suggested exercises! There won't be many, but will be essential to follow the discussions we will have. Days 2 and 3 are problem oriented based on these exercises.

-   Ask questions! Don't take home unresolved doubts, they stink!

-   The workshop material has **a** **lot!** Do you need to learn everything? No! We conveniently placed a main message at each section especially for you!

-   We won't be focusing on codes during explanations, but all code is provided in the material. The practical component comes mostly from the exercises, another good reason to work with the recommended questions. The problems you will encounter will most likely be the ones discussed in the next day.

-   Check out the notation guide and glossary beforehand. They will be useful for you!

<br><br>

## What will we learn?

**Day 1: Linear models and parameter estimation**

-   Linear vs Nonlinear models - What is the difference?

-   Parameter estimation:

    -   Loss-function approach

    -   Likelihood-based approach

    -   Bayesian approach

-   Why Bayesian inference may be useful?

**Day 2: Nonlinear models**

-   Linearization - What is going on behind the scenes?

-   Types of models: Mechanistic vs Phenomenological

-   Identifiability of parameters - Deeply related to estimation

-   Working with weakly identifiable parameters using likelihood-based approach

    -   Fixing parameters

    -   Linearization

**Day 3: Bayesian non-linear models**

-   Reasoning behind model selection - Mechanistic models and their meaning

-   A Bayesian example: Bayesian Disease Progress Curve

    -   Why use Bayesian?

-   Wrap-up

<br><br>

## Notation guide

We can write the same linear model using different notations, but preserving the same meaning. In this workshop, we will primarily use scalar and probabilistic notation for clarity. Vector and matrix notation are included here for reference. All notations below assume normally distributed errors.

$$
\varepsilon_i \sim N(0, \sigma^2)
$$

The Multivariate Normal distribution (MVN) is a compact way to represent multiple normally distributed observations together.

<br><br>

-   Scalar notation - Represents a single number

$$
y_i = \beta_0 + \beta_1*x_i + \varepsilon_i \\ \varepsilon_i \sim N(0, \sigma^2)
$$

<br><br>

-   Vector notation - Represents a vector of numbers

$$
\mathbf{y} = \beta_0 + \beta_1*\mathbf{x}+\boldsymbol{\varepsilon} \\[10pt] \begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_n \end{bmatrix} = \beta_0 + \beta_1*\begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_n \end{bmatrix} + \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ ... \\ \varepsilon_n \end{bmatrix} \\[15pt] \boldsymbol{\varepsilon} \sim MVN(\mathbf{0}, \sigma^2\mathbf{I})
$$

<br><br>

-   Matrix notation - Represents a matrix

$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} \\ \boldsymbol{\varepsilon} \sim MVN(\mathbf{0}, \sigma^2\mathbf{I}) 
$$

<br><br>

-   Probabilistic notation - Represents the distribution that we are assuming, a clear way to make our assumptions transparent. The expected value, or mean, can be expressed using any of the previous notations. To follow the same structure of the workshop, scalar notation will be exemplified.

    -   In distributional notation, see how we demonstrate that the deterministic mathematical equation influences the expectation/mean. The variance corresponds to the error term previously introduced as $\varepsilon_i \sim N(0, \sigma^2)$.

    -   This notation reads: Observations $y_i$ arise from a normal distribution with the expectation/mean $\mu_i$ controlled by our mathematical equation and a given variance $\sigma^2$. This variance relates to the observational level variance.

    -   This notation emphasizes that the model is a statement about how data are generated, combining a deterministic structure with stochastic variability.

$$
y_i \sim N(\mu_i, \sigma^2) \\ \mu_i = \beta_0 + \beta_1*x_i
$$

<br><br>

<center>Models are Assumptions + Structure + Uncertainty!</center>

## Glossary

Expectation = Mean - Where the distribution balances.

Uncertainty = Degree of doubt or unpredictability - How unsure we are about that estimated value. Can refer to predictions or parameters.

Estimation = The process of obtaining numerical values for the unknown quantities (parameters) of a model from observed data.

Inference = Interpreting estimated parameters in the context of the population or data-generating process (model). Inference is made about non-observable quantities and involves quantifying uncertainty.

Prediction = New data generated by the model, given covariates. A function of the model and stochastic variability.

Data generating process = Process that we believe to generate our data. Involves mathematical model + stochastic component.

Deterministic = Always produces the same result, not random.

Stochastic = Random.

Constraint = A restriction or boundary placed on model parameters. Often based on biological, physical, or logical consideration.

Random variable = Numerical value arising from a probability distribution. Formally: A function that connects the outcome of a random experiment to real numbers.

Support = Values it can assume.
