---
title: "Day 1"
author:
output: 
  html_document:
    code_download: TRUE
    code_folding: "hide"
    theme:
      bootswatch: journal
    toc: TRUE
    toc_float: TRUE
---

```{=html}
<style>
h1.title {
  text-align: center;
}
</style>
```

<br><br>

# Welcome to day 1!

-   In this workshop, we will try to mix theory and R coding, but our main focus will be on understanding the *why*. All applications should be driven by thinking first.

-   We will follow a step-wise structure:

    -   Day 1: Linear models

    -   Day 2: Non-linear models

    -   Day 3: Bayesian non-linear models

-   More importantly, **don't panic!** We will be learning together, slowly. We do not expect you to come into the workshop (or leave it) knowing everything.

-   Our goal is to provide an immersion into the topic and some initial guidance, so you can move forward with your own projects and continued learning.

-   Please feel free to stop me and ask questions at any time. I will answer to the best of my knowledge, and if I donâ€™t know the answer, either Trevor will jump in or I will make sure to come back to you with a clear explanation later.

<br><br>

# Day 1 - Linear models

<div>

**Outline:**

1.  What is and what isn't a linear model

2.  Estimation and Inference: A linear model example using:

    -   Loss function approach

    -   Likelihood-based approach

    -   Bayesian approach

3.  When Bayesian improves

4.  Practice questions

    -   Fixation day 1

    -   Pre-day 2

**What is day 1 about?**

Understanding linear models and estimation conceptually

</div>

<br><br>

## 1. What is and what isn't a linear model

### Recap on linear models

Recall the famous intercept-and-slope model, written in the "model equation form"[^1]:

[^1]: We are using scalar notation throughout this class.

$$
y_i = \beta_0 + x_i\beta_1 + \varepsilon_i
$$

Here we have the observed value of the $i$th observation ($y_i$), the intercept ($\beta_0$), representing the level of $y$ when $x$ = 0, the slope ($\beta_1$), which for a continuous $x_i$, it represents the change in $y$ given a one unit change in $x$, and the residual ($\varepsilon_i$) that represents the different between the observation and the model line for each point.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2026)
data <- data.frame(
  y = runif(100, 1, 15),
  x = seq(1, 100, by = 1)
)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
data_lm <- data
lin_fit <- lm(y ~ x, data = data_lm)
data_lm$pred <- predict(lin_fit)
plot(data_lm$x, data_lm$pred, type = "l", ylab = "y", xlab = "x")
```

### Now, what makes a model linear?

<center>**A model is linear if it is linear in its parameters.**</center>

Other examples of linear models are:

**Polynomial regression:**

Extend the linear regression in a way that the relationship between the predictors and the response is non-linear. A polynomial function is a special case of base function[^2].

[^2]: Base function is a known function of the predictor ($x$) that gets its own coefficient in a linear model. In short, a mathematical transformation of the predictor. For example, the linear model $y_i = \beta_0 + \beta_1x_i + \varepsilon_i$ can be written as $y_i = \beta_0 * 1 + \beta_1 * x + \varepsilon_i$, in which the base functions ($b$) are $b_0(x) = 1$ and $b_1(x) = x$. For polynomial regression, for example, the base function could be represented as $b_d(x_i) = x_i^d$, with $d$ being the polynomial degree.

$$
y_i = \beta_0 + \beta_1x_i+\beta_2x_i^2+\varepsilon_i
$$

```{r, message=FALSE, warning=FALSE, echo=FALSE}
data_poly <- data
poly_fit <- lm(y ~ poly(x, 2), data = data_poly)
data_poly$pred <- predict(poly_fit)
plot(data_poly$x, data_poly$pred, type = "l", ylab = "y", xlab = "x")
```

**Regression splines:**

In this type of model, $x$ is partitioned by $K$ points and within each space between these points, a polynomial model is fitted. Splines are a special case of piecewise degree-d polynomial, in which a constraint ensures that it is continuous. This means that at each "knot" ($K$), that define a breaks in $x$, the line remains continuous. Simple piecewise polynomial regression exist, but they do not ensure this continuity.

$$
y_i = \beta_0 + \beta_1b_1(x_i) + \beta_2b_{K + 2}(x_i) + \varepsilon_i
$$

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(splines) # Functions for working with regression splines
data_spl <- data
spl_fit <- lm(y ~ bs(x, df = 6), data = data_spl)
data_spl$pred <- predict(spl_fit)
plot(data_spl$x, data_spl$pred, type = "l", ylab = "y", xlab = "x")
```

<center>

Yes, this is linear!

Linearity is about the geometry of the parameter space, not about the shape of the curve in the data.

Shape $\neq$ Linearity

Linear models $\neq$ Simplicity

</center>

**Simple rule: A model is linear if:**

-   **All unknown parameter only multiplies known quantities**

-   **All parameters are added together**

-   **All parameters are outside nonlinear functions.**

$$
\mathbf{\hat{y}} = \sum_i\beta_ib_i(\mathbf{x})
$$

### What about nonlinear models?

In simple terms, in nonlinear models, parameters are inside nonlinear functions.

Example:

<center>**Exponential model:**</center>

$$
y_i = \beta_0*e^{\beta_1x_i} + \varepsilon_i
$$

```{r, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2026)
data_exp <- data.frame(
  x = seq(1, 100, by = 1)
)
data_exp$y = runif(1, 0, 20)*exp(runif(1, 0.005, 0.075)*data_exp$x + runif(100, 0, 5))

exp_fit <- nls(y ~ a*exp(b*x), data = data_exp, start = list(a = 13, b = 0.1))
data_exp$pred <- predict(exp_fit)
plot(data_exp$x, data_exp$pred, type = "l", ylab = "y", xlab = "x")
#points(data_exp$x, data_exp$y)
```

Checklist:

-   All unknown parameters only multiply known quantities? X -\> $\beta_0*e^{\beta_1x_i}$

-   All parameters are added together? X -\> $\beta_0*e^{\beta_1x_i}$

-   All parameters are outside nonlinear functions? X -\> $e^{\beta_1x_i}$

How do we work with non-linear models?

-   Fit it as it is.

-   Linearization.

    -   More about both on day 2!

<br><br>

### How do we chose a a model?

-   What is your research question?

    -   Pattern (phenomenological) vs Process (mechanistic)

-   What do you know about what you are investigating?

-   What do you want from the parameters?

    -   Focus: Inference? Prediction? Both?

<br><br>

## 2. Estimation and Inference: A linear model example

Estimation: Getting actual numbers for the model parameters

Inference: Making sense of these numbers regarding the population being studied

<br><br>

### Example: Understanding nitrogen balance in grain legumes

Details from [Palmero et al. 2024](https://link.springer.com/article/10.1186/s13007-024-01261-9)

A simple intercept-slope linear model:

$$
y_i = \beta_0 + \beta_1x_i + \varepsilon
$$

in which:

-   $y_i$ represents the proportion (%) of nitrogen (N) derived from the atmosphere (Ndfa)

-   $x_i$ represents the partial N balance (fixed N - removed N by grains) in kg/ha

The research question: Which Ndfa ($y_i$) leads to a neutral N balance ($x_i = 0$)?

```{r, message=FALSE, warning=FALSE}
# Data preparation
df <- read.csv("Palmero_data.csv")
df_a <- df[which(df$Scenario == "Scenario A"),]
plot(df_a$Ndfa, df_a$PartNbalance, ylab = "Partial N balance (kg/ha)", xlab = "Ndfa (%)")
abline(a = 0, b = 0, col = "gold", lwd = 3)
```

### I. Loss function approach

**How does it works?**

```{r, message=FALSE, warning=FALSE, echo=FALSE}
x <- c(1, 2, 3)
y <- c(1, 2, 2.5)
plot(x, y, pch = 16, xlab = "x", ylab = "y")
abline(a = 0.8, b = 0.4, lwd = 2)
yhat <- 0.8+0.4*x
segments(x0 = x, y0 = y, x1 = x, y1 = yhat, lty = 2)
```

Loss function: How we calculate the distance between observed and predicted by the model.

-   Several different options, the most famous: Least Square - $\sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_i)^2$

This approach produces a line of best fit based on values for $\beta_0$ and $\beta_1$ that minimizes the distance.

<center>**No uncertainty around estimation, no p-values, no inference, only parameter values!**</center>

<br><br>

**For our example:**

Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance ($\theta$).

$$
y_i = \beta_0 + \beta_1x_i + \varepsilon_i
$$

Theta is a derived quantity[^3] calculated as:

[^3]: Derived quantities are functions of model parameters

$$
Ndfa = -\frac{\beta_0}{\beta_1}
$$

```{r, message=FALSE, warning=FALSE}
# Fit the model
m1 <- lm(PartNbalance ~ Ndfa, data = df_a)

# Ndfa to achieve neutral N balance
b0hat_loss <- as.numeric(coef(m1)[1]) # extract the intercept
b1hat_loss <- as.numeric(coef(m1)[2]) # extract the slope

thetahat_loss <- -b0hat_loss/b1hat_loss # calculate theta

thetahat_loss
```

Visual representation of our line of best fit and $\theta$:

```{r, message=FALSE, warning=FALSE}
plot(df_a$Ndfa, df_a$PartNbalance, xlab = "Ndfa (%)", ylab = "Partial N balance (kg/ha)")
abline(a = 0, b = 0, col = "gold", lwd = 3)
abline(m1, col = "red", lwd = 3)
abline(v = thetahat_loss, lwd = 3, lty = 2, col = "green")
```

<br><br>

### II. Likelihood-based approach

**How does it work?**

Instead of minimizing the distance, we now maximize the likelihood function.

-   The question is: "If these parameter values were true, how plausible is the data I actually observe?".

-   Likelihood is about explaining the data you already have[^4].

-   No more a line of best fit, but an expectation!

[^4]: Finding the parameter values under which the observed data are most plausible according to the assumed data-generating process.

To define a likelihood, we must introduce assumptions about how the data were generated. We make a **distributional assumption** for the random error term:

$$
\varepsilon_i \sim N(0, \sigma^2)
$$

-   This distributional assumption is about how the data is generated, it means we believe the deviations between the observed data and the model predictions arise from a normal distribution with mean 0 and variance $\sigma^2$.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)

x <- c(1, 2, 3)
y <- c(1, 2, 2.5)

b0 <- 0.8
b1 <- 0.4
sigma <- 0.6

mu <- b0 + b1*x

L <- prod(dnorm(y, mean = mu, sd = sigma))

dplot <- data.frame(x = x, y = y, mu = mu)

y_grid <- seq(min(y, mu) - 3*sigma, max(y, mu) + 3*sigma, length.out = 300)

scale <- 0.35
curve_df <- data.frame()

for(i in seq_along(x)){
  dens <- dnorm(y_grid, mean = mu[i], sd = sigma)
  temp <- data.frame(
    x_curve = x[i] + scale*dens/max(dens),
    y_curve = y_grid,
    id = factor(i)
  )
  
  curve_df <- rbind(curve_df, temp)
}

max_dens <- max(dnorm(y_grid, mean = mu, sd = sigma))

dplot$x_end <- x + scale * dnorm(y, mean = mu, sd = sigma) / max_dens

line_df <- data.frame(x = seq(min(x)-1, max(x)+1, length.out = 200))
line_df$y <- b0 + b1*line_df$x

ggplot() +
  geom_path(data = curve_df, aes(x = x_curve, y = y_curve, group = id),
            linewidth = 0.8, color = "grey40") +
  geom_line(data = line_df, aes(x = x, y = y), linewidth = 1.2) +
  geom_point(data = dplot, aes(x = x, y = y), size = 3) +
  geom_segment(data = dplot,
               aes(x = x, xend = x_end, y = y, yend = y),
               linetype = "dashed", linewidth = 0.8, color = "grey30") +
  labs(x = "x", y = "y") +
  coord_cartesian(clip = "off") +
  theme_classic(base_size = 14)
```

<center>**Here we have full likelihood based statistical inference (p-values, confidence intervals, etc).**</center>

<br><br>

**For our example:**

Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance ($\theta$).

$$ y_i = \beta_0 + \beta_1x_i + \varepsilon_i \\ \varepsilon_i \sim N(0, \sigma^2) $$

```{r, message=FALSE, warning=FALSE}
library(nlme)
# Fit the model
m2 <- gls(PartNbalance ~ Ndfa, data = df_a, method = "ML")

# Ndfa to achieve neutral N balance
b0hat_ml <- as.numeric(coef(m2)[1]) # extract the intercept
b1hat_ml <- as.numeric(coef(m2)[2]) # extract the slope

thetahat_ml <- -b0hat_ml/b1hat_ml # calculate theta

thetahat_ml
```

Here we can construct confidence intervals by approximating the standard errors using delta method[^5].

[^5]: The Delta Method approximates the uncertainty of a transformed estimator (derived quantity), assuming it was originally approximately normal.

```{r, message=FALSE, warning=FALSE}
library(msm)
theta_se <- deltamethod(~-x1/x2, mean = coef(m2), cov = vcov(m2))
theta_ci <- c(thetahat_ml-1.96*theta_se, thetahat_ml+1.96*theta_se)
theta_ci
```

Visual representation of our expected values and $\hat{\theta}$:

```{r, message=FALSE, warning=FALSE}
plot(df_a$Ndfa, df_a$PartNbalance, xlab = "Ndfa (%)", ylab = "Partial N balance (kg/ha)")
abline(a = 0, b = 0, col = "gold", lwd = 3)
abline(m2, col = "red", lwd = 3)
abline(v = thetahat_ml, lwd = 3, lty = 2, col = "green")
abline(v = theta_ci[1], lwd = 1, lty = 2, col = "green")
abline(v = theta_ci[2], lwd = 1, lty = 2, col = "green")
```

What are we gaining here?

-   More complexity;

-   Uncertainty around estimated $\hat{\theta}$;

-   We could also explore uncertainty for the expected value (red line).

<br><br>

### III. Bayesian approach

**How does it work?**

Now, we do not focus on the most likely value of the parameter, we focus on it's whole distribution, all values it can assume.

In this approach, we have additional assumptions concerning our unknown parameters.

<center>"Bayesian statistical models provide a useful way to obtain inference and predictions for **unobserved quantities** [parameters] in nature based on a solid foundation of mathematical rules pertaining to probability." ()</center>

Through Bayesian approach, it is very straight forward to account for and explicitly demonstrate uncertainty.

We use **prior knowledge** about the system, to inform our parameters to be estimated.

Bayesian estimation does not contradict Maximum Likelihood, it extends it!

<center>**More on day 3**</center>

```{r, message=FALSE, warning=FALSE, echo=FALSE}
set.seed(2026)

bay_ml <- rnorm(1000, 0.8, 5)

par(mfrow = c(1, 2))

hist(bay_ml, freq = FALSE, xlab = "x", main = "Bayesian")
abline(v = 0.8, lwd = 2, col = "red")
plot(x = c(min(bay_ml), max(bay_ml)), y = c(0, 0), type = "n", xlab = "x", ylab = "", main = "ML")
abline(v = 0.8, lwd = 2, col = "red")

par(mfrow = c(1, 1))
```

**For our example:**

Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance ($\theta$).

$$ y_i = \beta_0 + \beta_1x_i + \varepsilon_i \\ \varepsilon_i \sim N(0, \sigma^2)$$

We will re-write this model, using a different notation:

$$
\mathbf{y} \sim N(\boldsymbol{\beta_0} + \boldsymbol{\beta_1}\mathbf{x}, \boldsymbol{\sigma}^2) \\ \boldsymbol{\beta_0} \sim N(0, 10^6) \\ \boldsymbol{\beta_1} \sim N(0, 10^6) \\ \boldsymbol{\sigma} \sim Gamma(2.5, 0.05)
$$\
This notation helps to clearly identify our assumptions!

```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=FALSE}
# Writting the model

sink("model.txt")
cat("
    model {
    
    # Likelihood
    for(i in 1:N){
      y[i] ~ dnorm(mu[i], tau)
      mu[i] <- b0 + b1*(x[i])
    }
    
    #x.bar <- mean(x)
    sigma <- 1/sqrt(tau)
    
    # Priors
    b0 ~ dnorm(0, 1/10^6)
    b1 ~ dnorm(0, 1/10^6)
    tau ~ dgamma(2.5, 0.05)
    
    # Derived quantity
    theta <- -b0/b1
    }
    ")
sink()
```

```{r, message=FALSE, warning=FALSE, results='hide'}
library(rjags)
library(coda)

set.seed(2026)

y <- df_a$PartNbalance
x <- df_a$Ndfa

# JAGS code
data_line <- list("y" = y, "x" = x, "N" = nrow(df_a))
initials <- list(
  list("b0" = 0.1, "b1" = 0.1, "sigma" = 0.1)
)

model_string <- "model {
    
    # Likelihood
    for(i in 1:N){
      y[i] ~ dnorm(mu[i], tau)
      mu[i] <- b0 + b1*(x[i])
}
    
    # Priors
    b0 ~ dnorm(0, 1/10^6)
    b1 ~ dnorm(0, 1/10^6)
    sigma ~ dgamma(2.5, 0.05)
    
    # Derived quantity
    theta <- -b0/b1
    tau <- 1/(sigma^2)
}"

m3 <- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1)

update(m3, n.iter = 1000)

samples <- coda.samples(m3, variable.names = c("b0", "b1", "tau", "theta", "sigma"), n.iter = 1000)

#summary(samples)
#plot(samples)

# Extract draws and calculate expected line
draws <- as.matrix(samples)

b0_hat <- mean(draws[, "b0"])
b1_hat <- mean(draws[, "b1"])

pred_mean <- b0_hat + b1_hat*x
```

```{r, message=FALSE, warning=FALSE}
par(mfrow = c(2, 2))

hist(draws[, "b0"], freq = FALSE, main = "Posterior dist. b0", xlab = "b0")
abline(v = mean(draws[, "b0"]), lwd = 2, col = "red")

hist(draws[, "b1"], freq = FALSE, main = "Posterior dist. b1", xlab = "b1")
abline(v = mean(draws[, "b1"]), lwd = 2, col = "red")

hist(draws[, "theta"], freq = FALSE, main = "Posterior dist. theta", xlab = "theta")
abline(v = mean(draws[, "theta"]), lwd = 2, col = "red")

par(mfrow = c(1, 1))
```

```{r, message=FALSE, warning=FALSE}
theta_CrI <- quantile(draws[, "theta"], c(0.025, 0.975))

plot(df_a$Ndfa, df_a$PartNbalance, xlab = "Ndfa (%)", ylab = "Partial N balance (kg/ha)")
abline(a = 0, b = 0, col = "gold", lwd = 3)
lines(df_a$Ndfa, pred_mean, col = "red", lwd = 2)
abline(v = mean(draws[, "theta"]), lwd = 3, lty = 2, col = "green")
abline(v = theta_CrI[1], lwd = 1, lty = 2, col = "green")
abline(v = theta_CrI[2], lwd = 1, lty = 2, col = "green")
```

<br><br>

#### What changes?

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(knitr)
library(kableExtra)

df_table <- data.frame(
  Method = c("LS", "ML", "Bay"),
  b0 = c(round(b0hat_loss, 2), round(b0hat_ml, 2), round(mean(draws[, "b0"]), 2)),
  b1 = c(round(b1hat_loss, 2), round(b1hat_ml, 2), round(mean(draws[, "b1"]), 2)),
  theta = c(round(thetahat_loss, 2), round(thetahat_ml, 2), round(mean(draws[, "theta"]), 2)),
  theta_l = c(" ", round(theta_ci[1], 2), round(theta_CrI[1], 2)),
  theta_u = c(" ", round(theta_ci[2], 2), round(theta_CrI[2], 2))
)

knitr::kable(df_table, align = "c") %>%
  kable_styling(full_width = TRUE, position = "center")
```

**Why?**

-   Simple model

-   Lot's of observations - No deficiency on informational content

<br><br>

## 3. Why use Bayesian?

```{r, message=FALSE, warning=FALSE}
df_b <- df[which(df$Scenario == "Scenario B"),]
plot(df_b$Ndfa, df_b$PartNbalance, xlab = "Ndfa (%)", ylab = "Partial N balance (kg/ha)", ylim = c(-100, 100), xlim = c(0, 110))
abline(a = 0, b = 0, lwd = 3, col = "gold")
```

You should be concerned about uncertainty here! Let's try a likelihood approach:

```{r, message=FALSE, warning=FALSE}
# Fit the model
m4 <- gls(PartNbalance ~ Ndfa, data = df_b, method = "ML")

# Ndfa to achieve neutral N balance
b0hat_ml2 <- as.numeric(coef(m4)[1]) # extract the intercept
b1hat_ml2 <- as.numeric(coef(m4)[2]) # extract the slope

thetahat_ml2 <- -b0hat_ml2/b1hat_ml2 # calculate theta

theta_se2 <- deltamethod(~ -x1/x2, mean = coef(m4), cov = vcov(m4))
theta_ci2 <- c(thetahat_ml2-1.96*theta_se2, thetahat_ml2+1.96*theta_se2)

plot(df_b$Ndfa, df_b$PartNbalance, xlim = c(0, 110), ylim = c(-100, 100), xlab = "Ndfa (%)", ylab = "Partial N Balance (kg/ha)")
abline(a = 0, b = 0, lwd = 3, col = "gold")
abline(m4, lwd = 3, col = "red")
abline(v = thetahat_ml2, lwd = 3, lty = 2, col = "green")
abline(v = theta_ci2[1], lwd = 1, lty = 2, col = "green")
abline(v = theta_ci2[2], lwd = 1, lty = 2, col = "green")
```

How about Bayesian?

```{r, message=FALSE, warning=FALSE, results='hide'}
set.seed(2026)

y <- df_b$PartNbalance
x <- (df_b$Ndfa)/100

# JAGS code
data_line <- list("y" = y, "x" = x, "N" = nrow(df_b))
initials <- list(
  list("theta" = 0.1, "b1" = 0.1, "sigma" = 0.1)
)

model_string <- "model {
    
    # Likelihood
    for(i in 1:N){
      y[i] ~ dnorm(mu[i], tau)
      mu[i] <- -b1*theta + b1*(x[i])
}
    
    # Priors
    b1 ~ dgamma(1.6, 0.8)
    theta ~ dbeta(62.52, 14.06)
    sigma ~ dgamma(2.5, 0.05)
    
    # Derived quantity
    b0 <- -b1*theta
    tau <- 1/(sigma^2)
}"

m5 <- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1)

update(m5, n.iter = 1000)

samples2 <- coda.samples(m5, variable.names = c("b0", "b1", "tau", "theta", "sigma"), n.iter = 1000)

#summary(samples2)
#plot(samples2)

# Extract draws and calculate expected line
draws2 <- as.matrix(samples2)

theta_hat2 <- mean(draws2[, "theta"])
b1_hat2 <- mean(draws2[, "b1"])

pred_mean2 <- b1_hat2* + b1_hat2*x
```

```{r, message=FALSE, warning=FALSE}
theta_CrI2 <- quantile(draws2[, "theta"], c(0.025, 0.975))

plot(df_b$Ndfa, df_b$PartNbalance, xlab = "Ndfa (%)", ylab = "Partial N balance (kg/ha)", ylim = c(-100, 100), xlim = c(0, 110))
abline(a = 0, b = 0, col = "gold", lwd = 3)
#lines(df_b$Ndfa, pred_mean2, col = "red", lwd = 2)
abline(v = (mean(draws2[, "theta"]))*100, lwd = 3, lty = 2, col = "green")
abline(v = (theta_CrI2[1])*100, lwd = 1, lty = 2, col = "green")
abline(v = (theta_CrI2[2])*100, lwd = 1, lty = 2, col = "green")
```

Why?

-   Deficient information about $\theta$ in this dataset.

-   Bayesian offers the opportunity to utilize prior information and a constraint $\hat{\theta}$.

-   We treat $\theta$ as a **random variable.**

-   Similar assumptions across approaches.

<br><br>

## 4. Practice for day 1

-   Sample code is provided for some questions. Code used on the class examples can also be used to work on this practice.

#### Fixation day 1

**I.** Using the field pea and white lupine data, experiment fitting the simple intercept-slope linear model ($y_i = \beta_0 + \beta_1*xi + \varepsilon_i$) using the likelihood approach.

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Data
## Field pea data (Scenario A)
fp <- df[which(df$Scenario == "Scenario A"),]

## White lupine data (Scenario B)
wl <- df[which(df$Scenario == "Scenario B"),]
```

**II.** Calculate the derived quantity $\hat{\theta}$, and use delta method and bootstrap to obtain confidence intervals. Compare the intervals between the two datasets.

-   Bootstrap will be revisited on day 3, please, have a look at it beforehand!

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Bootstrap for field pea (fp)

library(nlme) # gls function for model fit

set.seed(2026) # As we are working with a random (stochastic) process, we need to set seed to be able to reproduce the results

ite <- 1000 # How many iterations (repetitions of the process) we will have

n <- nrow(fp) # Sample size to be collected (= df size)

theta.save <- seq(0, ite-1) # Save theta calculated for each sample

# Custom bootstrap algorithm
for(i in 1:ite){
  
  sample <- fp[sample(nrow(fp), n, replace = TRUE), ]
  
  fit <- gls(PartNbalance ~ Ndfa, data = sample, method = "ML")
  
  theta <- -coef(fit)[1]/coef(fit)[2]
  
  theta.save[i] <- theta
  
}

# Plot the bootstrap distribution
hist(theta.save, freq = FALSE, main = "Bootstrap distribution of theta", xlab = "theta")

# 95% CI
quantile(theta.save, probs = c(0.025, 0.975))
```

Steps:

-   Sample n elements with replacement from the original data

-   For every sample calculate the desired statistic (here $\theta$)

-   Repeat steps 1 and 2 n times (iterations = ite) and save the calculated statistic

-   Plot the calculated statistics which forms the bootstrap distribution

-   Using the bootstrap distribution of desired statistics calculate the 95% CI

#### Pre-day 2 - Non-linear models

**I.** Fit an logistic model to the data of plant disease progress over time in "nonlin.csv".

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Data
nl_df <- read.csv("nonlin.csv")
nl_24 <- nl_df[which(nl_df$year == 2024 & nl_df$var == "a"),]
nl_24$p_sev <- nl_24$severity/100
nl_23 <- nl_df[which(nl_df$year == 2023 & nl_df$var == "a"),]
nl_23$p_sev <- nl_23$severity/100
```

**I.a.** Use linearization to fit a linear model for 2024. We will use a logit transformation:

$$
logit(y_i) = \beta_0 + \beta_1*x_i + \varepsilon_i
$$

Where $\beta_1 \sim r$ from the non-linear form.

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Example for 2024

# For this, we assume K = 1
nl_24$l_sev <- (nl_24$p_sev/(1-nl_24$p_sev))

# Fit the linear model to the logit transformed y
m1 <- lm(l_sev ~ time, data = nl_24)

# Make predictions
logit_pred <- data.frame(
  time = seq(min(nl_24$time), max(nl_24$time), 1))

logit_pred$pred <- predict(m1, newdata = logit_pred)

# Plot predictions
plot(nl_24$time, nl_24$l_sev, ylab = "Logit(y)", xlab = "Days", ylim = c(-3, 19))
lines(logit_pred$time, logit_pred$pred, lwd = 3, col = "red")
```

**I.b.** Use the nls function. First, fit for the year of 2024, then, do it for the year of 2023. Make a plot with predictions. Save models for the next question.

$$
y_i = \frac{K}{1 + (\frac{K - y_0}{y_0})e^{-rt_i}} + \varepsilon_i
$$

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Example for 2024

# Fit the logistic model
m2 <- nls(p_sev ~ K/(1 + ((K-y0)/y0)*exp(-r*time)), data = nl_24, start = list(K = 0.7, y0 = 0.001, r = 0.2))

# Predict using the logistic curve
nl_pred <- data.frame(
  time = seq(min(nl_24$time), max(nl_24$time), 1))

nl_pred$pred <- predict(m2, newdata = nl_pred)

# Plot predictions
plot(nl_24$time, nl_24$p_sev, ylab = "Severity (prop.)", xlab = "Days", ylim = c(-0.1, 1))
lines(nl_pred$time, nl_pred$pred, lwd = 3, col = "red")
```

**II.** Extract model parameters. Check the parameter meaning below. Check the Wald CI. Are values making sense?

-   Disease severity: Percentage of tissue area affected by the disease (in this case, percentage of the leaf area affected)

-   Logistic disease progress curve model parameter meaning:

    -   $K$ - Maximum disease potential - Maximum severity level

    -   $r$ - Disease rate of growth - How fast it grows

    -   $y_0$ - Initial disease level

```{r, message=FALSE, warning=FALSE, eval=FALSE}
summary(m2) # Overall model summary

# Example - For K:
## Extract the estimated value
K <- as.numeric(coef(m2)[1])

## Calculate Wald Confidence Interval for the parameter estimate
K_sd <- sqrt(vcov(m2)[1])
K_lb <- K-1.96*K_sd # CI Lower
K_ub <- K+1.96*K_sd # CI Upper

# Notes for you:
## nls in R uses least squares, a loss function approach. Recall: Minimize the loss function. 

## To get CI here we are making additional assumptions that we do not make for loss function approach: iid normal residuals.

## This is possible because LS = ML under iid Normal errors with constant variance.

## Why 1.96? It represents the 97.5th percentile of the std N distribution.
```

**III.** Try fitting a polynomial to this data. Does the predicted curve make sense visually?

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Example for 2024

# Fit the model
m3 <- lm(p_sev ~ poly(time, degree = 3), data = nl_24)

# Make predictions
poly_pred <- data.frame(
  time = seq(min(nl_24$time), max(nl_24$time), 1))

poly_pred$pred <- predict(m3, newdata = poly_pred)

# Plot predictions
plot(nl_24$time, nl_24$p_sev, ylab = "Severity (prop.)", xlab = "Days", ylim = c(-0.1, 1))
lines(poly_pred$time, poly_pred$pred, lwd = 3, col = "red")
```

About the parameters, are they still carrying biological meaning? How fast is the disease growing?

```{r, message=FALSE, warning=FALSE, eval=FALSE}
summary(m3)
```

# Notes

Add a model notation portion

Notes for day 2:\
Parameter meaning - Mechanistic models

Parameter estimation

"Non-linear models are unforgiving when the data do not inform the parameters"

Problems of linearization

Notes for day 3:

Variety comparison on day 3!

At this point you maybe is thinking, why are we doing this? Why just going with logistic? Shouldn't we be testing other models and comparing them? Why not exponential, monomolecular, others??
