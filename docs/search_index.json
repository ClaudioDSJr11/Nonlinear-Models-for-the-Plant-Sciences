[["index.html", "Non-Linear Models for the Plant Sciences About", " Non-Linear Models for the Plant Sciences Claudio Dias da Silva Jr &amp; Trevor Hefley 2026-02-04 About This material is the base for the workshop Nonlinear Models for the Plant Sciences taught at Kansas State University on the Spring of 2026. This workshop was develop as a collaboration from the Department of Statistics, Department of Plant Pathology and the Institute for Digital Agriculture &amp; Advanced Analytics (ID3A) as part of the Advanced Analytics Workshops series. Motivation: Non-linear models are widely used in plant sciences. Just like linear models, non-linear models can be fitted to data to enable statistical inference, which is required for scientific investigations and can aid when making management decisions. For example, logistic disease progression curves are a non-linear model used to understand plant disease epidemics, and statistical estimation and inference are needed to connect field data to management decisions (e.g., when to spray fungicide). “Non-Linear Models for the Plant Sciences” is designed to help you develop an understanding of non-linear models and how they fit data using common statistical techniques such as maximum likelihood estimation and Bayesian estimation. This workshop is co-hosted by the Departments of Statistics and Plant Pathology. "],["pre-workshop-guide.html", "Pre-workshop guide How to make the most of this workshop What will we learn? Notation guide Glossary", " Pre-workshop guide How to make the most of this workshop Work on suggested exercises! There won’t be many, but will be essential to follow the discussions we will have. Days 2 and 3 are problem oriented based on these exercises. Ask questions! Don’t take home unresolved doubts, they stink! The workshop material has a lot! Do you need to learn everything? No! We conveniently placed a main message at each section especially for you! We won’t be focusing on codes during explanations, but all code is provided in the material. The practical component comes mostly from the exercises, another good reason to work with the recommended questions. The problems you will encounter will most likely be the ones discussed in the next day. Check out the notation guide and glossary beforehand. They will be useful for you! What will we learn? Day 1: Linear models and parameter estimation Linear vs Nonlinear models - What is the difference? Parameter estimation: Loss-function approach Likelihood-based approach Bayesian approach Why Bayesian inference may be useful? Day 2: Nonlinear models Linearization - What is going on behind the scenes? Types of models: Mechanistic vs Phenomenological Identifiability of parameters - Deeply related to estimation Working with weakly identifiable parameters using likelihood-based approach Fixing parameters Linearization Day 3: Bayesian non-linear models Reasoning behind model selection - Mechanistic models and their meaning A Bayesian example: Bayesian Disease Progress Curve Why use Bayesian? Wrap-up Notation guide We can write the same linear model using different notations, but preserving the same meaning. In this workshop, we will primarily use scalar and probabilistic notation for clarity. Vector and matrix notation are included here for reference. All notations below assume normally distributed errors. \\[ \\varepsilon_i \\sim N(0, \\sigma^2) \\] The Multivariate Normal distribution (MVN) is a compact way to represent multiple normally distributed observations together. Scalar notation - Represents a single number \\[ y_i = \\beta_0 + \\beta_1*x_i + \\varepsilon_i \\\\ \\varepsilon_i \\sim N(0, \\sigma^2) \\] Vector notation - Represents a vector of numbers \\[ \\mathbf{y} = \\beta_0 + \\beta_1*\\mathbf{x}+\\boldsymbol{\\varepsilon} \\\\[10pt] \\begin{bmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n \\end{bmatrix} = \\beta_0 + \\beta_1*\\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ ... \\\\ \\varepsilon_n \\end{bmatrix} \\\\[15pt] \\boldsymbol{\\varepsilon} \\sim MVN(\\mathbf{0}, \\sigma^2\\mathbf{I}) \\] Matrix notation - Represents a matrix \\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\\\ \\boldsymbol{\\varepsilon} \\sim MVN(\\mathbf{0}, \\sigma^2\\mathbf{I}) \\] Probabilistic notation - Represents the distribution that we are assuming, a clear way to make our assumptions transparent. The expected value, or mean, can be expressed using any of the previous notations. To follow the same structure of the workshop, scalar notation will be exemplified. In distributional notation, see how we demonstrate that the deterministic mathematical equation influences the expectation/mean. The variance corresponds to the error term previously introduced as \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). This notation reads: Observations \\(y_i\\) arise from a normal distribution with the expectation/mean \\(\\mu_i\\) controlled by our mathematical equation and a given variance \\(\\sigma^2\\). This variance relates to the observational level variance. This notation emphasizes that the model is a statement about how data are generated, combining a deterministic structure with stochastic variability. \\[ y_i \\sim N(\\mu_i, \\sigma^2) \\\\ \\mu_i = \\beta_0 + \\beta_1*x_i \\] Models are Assumptions + Structure + Uncertainty! Glossary Expectation = Mean - Where the distribution balances. Uncertainty = Degree of doubt or unpredictability - How unsure we are about that estimated value. Can refer to predictions or parameters. Estimation = The process of obtaining numerical values for the unknown quantities (parameters) of a model from observed data. Inference = Interpreting estimated parameters in the context of the population or data-generating process (model). Inference is made about non-observable quantities and involves quantifying uncertainty. Prediction = New data generated by the model, given covariates. A function of the model and stochastic variability. Data generating process = Process that we believe to generate our data. Involves mathematical model + stochastic component. Deterministic = Always produces the same result, not random. Stochastic = Random. Constraint = A restriction or boundary placed on model parameters. Often based on biological, physical, or logical consideration. Random variable = Numerical value arising from a probability distribution. Formally: A function that connects the outcome of a random experiment to real numbers. Support = Values it can assume. "],["welcome-to-day-1.html", "Welcome to day 1! Linear models and parameter estimation", " Welcome to day 1! In this workshop material, theory and R codes are mixed. Throughout the workshop, our main focus will be on understanding the why. We want to demonstrate that all codes in this workshop are used after deep understanding of what is going on behind them, after we thought about our models in depth. We will have the following structure: Day 1: Linear models and parameter estimation Day 2: Nonlinear models Day 3: Bayesian nonlinear models More importantly, don’t panic! We will be learning together, slowly. We do not expect you to come into the workshop (or leave it) knowing everything. Our goal is to provide an immersion into the topic and some initial guidance, so you can move forward with your own projects and continued learning. Please feel free to stop me and ask questions at any time. I will answer it to the best of my knowledge, and if I don’t know the answer, either Trevor will jump in or I will make sure to come back to you with a clear explanation in a later moment. Linear models and parameter estimation Outline: Section 1: What is and what isn’t a linear model Section 2: Estimation and Inference: A linear model example using: Loss function approach Likelihood-based approach Bayesian approach Why Bayesian may be useful? Questions Recommended - Base for Day 2 Optional - For practice only! What we expect you to learn on Day 1: What is a linear model and what is not There are different types of estimation, and they have different assumptions When it is wroth it to work with more assumption heavy approaches Section 1: What is and what isn’t a linear model Recap on linear models Recall the famous intercept-and-slope model, written in the “model equation form”1: \\[ y_i = \\beta_0 + x_i\\beta_1 + \\varepsilon_i \\] Here we have the observed value of the \\(i\\)th observation (\\(y_i\\)), the intercept (\\(\\beta_0\\)), representing the level of \\(y\\) when \\(x\\) = 0, the slope (\\(\\beta_1\\)), which for a continuous \\(x_i\\), it represents the change in \\(y\\) given a one unit change in \\(x\\), and the residual (\\(\\varepsilon_i\\)) that represents the different between the observation and the model line for each point. What makes a model linear? A model is linear if it is linear in its parameters. Other examples of linear models are: Polynomial regression: Extend the linear regression in a way that the relationship between the predictors and the response is nonlinear. A polynomial function is a special case of base function2. \\[ y_i = \\beta_0 + \\beta_1x_i+\\beta_2x_i^2+\\varepsilon_i \\] Regression splines: In this type of model, \\(x\\) is partitioned by \\(K\\) points and within each space between these points, a polynomial model is fitted. Splines are a special case of piecewise degree-d polynomial, in which a constraint ensures that it is continuous. This means that at each “knot” (\\(K\\)), that define a breaks in \\(x\\), the line remains continuous. Simple piecewise polynomial regression exist, but they do not ensure this continuity. \\[ y_i = \\beta_0 + \\beta_1b_1(x_i) + \\beta_2b_{K + 2}(x_i) + \\varepsilon_i \\] Shape \\(\\neq\\) Linearity Linear models \\(\\neq\\) Simplicity Simple rule: A model is linear if: All unknown parameter only multiplies known quantities All parameters are added together All parameters are outside nonlinear functions. \\[ \\mathbf{\\hat{y}} = \\sum_i\\beta_ib_i(\\mathbf{x}) \\] What about nonlinear models? In simple terms, in nonlinear models, parameters are inside nonlinear functions. Example: Exponential model: \\[ y_i = \\beta_0*e^{\\beta_1x_i} + \\varepsilon_i \\] Checklist: All unknown parameters only multiply known quantities? X -&gt; \\(\\beta_0*e^{\\beta_1x_i}\\) All parameters are added together? X -&gt; \\(\\beta_0*e^{\\beta_1x_i}\\) All parameters are outside nonlinear functions? X -&gt; \\(e^{\\beta_1x_i}\\) How do we work with nonlinear models? Fit it as it is. Linearization. More about both on day 2! How do we chose a a model? What is your research question? Pattern (phenomenological) vs Process (mechanistic) What do you know about what you are investigating? What do you want from the model? Focus: Inference? Prediction? Both? Highlight from Section 1: Shape \\(\\neq\\) Linearity: Not being a line does not mean it is not linear! Linear models \\(\\neq\\) Simplicity: Linear models can be quite complex (e.g. Splines). Model selection is more of a philosophical question, but some points might help you on the decision. Section 2: Estimation and Inference: A linear model example Estimation: The process of obtaining numerical values for the unknown quantities (parameters) of a model from observed data. Inference: Interpreting these estimated parameters in the context of the population or data-generating process (model). Inference is made on non-observable quantities of interest, like the parameters (e.g. \\(\\beta_1\\)) and involves quantifying uncertainty and drawing conclusions beyond the observed sample. Example: Understanding nitrogen balance in grain legumes Details from Palmero et al. 2024 A simple intercept-slope linear model: \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\] in which: \\(y_i\\) represents the proportion (%) of nitrogen (N) derived from the atmosphere (Ndfa) \\(x_i\\) represents the partial N balance (fixed N - removed N by grains) in kg/ha The research question: Which Ndfa (\\(y_i\\)) leads to a neutral N balance (\\(x_i = 0\\))? Code # Data preparation df &lt;- read.csv(&quot;Palmero_data.csv&quot;) df_a &lt;- df[which(df$Scenario == &quot;Scenario A&quot;),] plot(df_a$Ndfa, df_a$PartNbalance, ylab = &quot;Partial N balance (kg/ha)&quot;, xlab = &quot;Ndfa (%)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) I. Loss function approach How does it works? Loss function: How we calculate the distance between observed and predicted by the model. Several different options, the most famous: Least Square - \\(\\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\) Least Square characteristics: Small residuals = Small loss / Large residuals = Large loss This approach produces a line of best fit based on values for \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the distance. At this point: No uncertainty around estimation, no p-values, no inference, only parameter values! A loss function by itself gives no inference. Inference comes from assumptions on the residuals (\\(\\varepsilon_i\\)) Here: \\(\\varepsilon_i = y_i - \\hat{y}_i\\) For our example: Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance (\\(\\theta\\)). \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\] Theta is a derived quantity3 calculated as: \\[ Ndfa = -\\frac{\\beta_0}{\\beta_1} \\] Code # Fit the model m1 &lt;- lm(PartNbalance ~ Ndfa, data = df_a) # Ndfa to achieve neutral N balance b0hat_loss &lt;- as.numeric(coef(m1)[1]) # extract the intercept b1hat_loss &lt;- as.numeric(coef(m1)[2]) # extract the slope thetahat_loss &lt;- -b0hat_loss/b1hat_loss # calculate theta thetahat_loss ## [1] 58.26843 Visual representation of our line of best fit and \\(\\theta\\): Code plot(df_a$Ndfa, df_a$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) abline(m1, col = &quot;red&quot;, lwd = 3) abline(v = thetahat_loss, lwd = 3, lty = 2, col = &quot;green&quot;) II. Likelihood-based approach How does it work? Instead of minimizing the distance, we now maximize the likelihood function. What likelihood do? According to the data-generating process (model + assumption) and data, estimate values of the parameters that are more plausible. No more a line of best fit, but an expectation/mean. To define a likelihood, we make a distributional assumption for the random error term. The most common: \\[ \\varepsilon_i \\sim N(0, \\sigma^2) \\] This distributional assumption is about how the data is generated, it means we believe the deviations between the observed data and the model predictions arise from a normal distribution with mean 0 and variance \\(\\sigma^2\\). Here we have full likelihood based statistical inference (p-values, confidence intervals, etc). For our example: Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance (\\(\\theta\\)). \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\\\ \\varepsilon_i \\sim N(0, \\sigma^2) \\] or \\[ y_i \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2) \\] Code library(nlme) # Fit the model m2 &lt;- gls(PartNbalance ~ Ndfa, data = df_a, method = &quot;ML&quot;) # Ndfa to achieve neutral N balance b0hat_ml &lt;- as.numeric(coef(m2)[1]) # extract the intercept b1hat_ml &lt;- as.numeric(coef(m2)[2]) # extract the slope thetahat_ml &lt;- -b0hat_ml/b1hat_ml # calculate theta thetahat_ml ## [1] 58.26843 Here we can construct confidence intervals by approximating the standard errors using delta method4. Code library(msm) theta_se &lt;- deltamethod(~-x1/x2, mean = coef(m2), cov = vcov(m2)) theta_ci &lt;- c(thetahat_ml-1.96*theta_se, thetahat_ml+1.96*theta_se) theta_ci ## [1] 52.88317 63.65370 Visual representation of our expected values and \\(\\hat{\\theta}\\): Code plot(df_a$Ndfa, df_a$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) abline(m2, col = &quot;red&quot;, lwd = 3) abline(v = thetahat_ml, lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = theta_ci[1], lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = theta_ci[2], lwd = 1, lty = 2, col = &quot;green&quot;) What changes from the loss function approach? More complexity; Uncertainty estimation (around estimated \\(\\hat{\\theta}\\)); We could also explore uncertainty for the expected value (red line). III. Bayesian approach How does it work? The focus now is not on the most likely value of the parameter, it is on the whole distribution, all values it can assume. In this approach, we have additional assumptions on the unknown parameters. “Bayesian statistical models provide a useful way to obtain inference and predictions for unobserved quantities [parameters] in nature based on a solid foundation of mathematical rules pertaining to probability.” (Hooten and Hefley, 2019) We use prior knowledge about the system, to inform our parameters to be estimated. Bayesian estimation does not contradict Maximum Likelihood, it extends it! More on day 3 For our example: Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance (\\(\\theta\\)). \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\\\ \\varepsilon_i \\sim N(0, \\sigma^2)\\] We will re-write this model, using distributional notation and adding our additional assumptions for other unknown quantities: \\[ \\mathbf{y} \\sim N(\\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1}\\mathbf{x}, \\boldsymbol{\\sigma}^2) \\\\ \\boldsymbol{\\beta_0} \\sim N(0, 10^6) \\\\ \\boldsymbol{\\beta_1} \\sim N(0, 10^6) \\\\ \\boldsymbol{\\sigma} \\sim Gamma(2.5, 0.05) \\] This notation helps to clearly identify our assumptions! Code library(rjags) library(coda) set.seed(2026) y &lt;- df_a$PartNbalance x &lt;- df_a$Ndfa # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;N&quot; = nrow(df_a)) initials &lt;- list( list(&quot;b0&quot; = 0.1, &quot;b1&quot; = 0.1, &quot;sigma&quot; = 0.1) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dnorm(mu[i], tau) mu[i] &lt;- b0 + b1*(x[i]) } # Priors b0 ~ dnorm(0, 1/10^6) b1 ~ dnorm(0, 1/10^6) sigma ~ dgamma(2.5, 0.05) # Derived quantity theta &lt;- -b0/b1 tau &lt;- 1/(sigma^2) }&quot; m3 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m3, n.iter = 1000) samples &lt;- coda.samples(m3, variable.names = c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;theta&quot;, &quot;sigma&quot;), n.iter = 1000) #summary(samples) #plot(samples) # Extract draws and calculate expected line draws &lt;- as.matrix(samples) b0_hat &lt;- mean(draws[, &quot;b0&quot;]) b1_hat &lt;- mean(draws[, &quot;b1&quot;]) pred_mean &lt;- b0_hat + b1_hat*x Code par(mfrow = c(2, 2)) hist(draws[, &quot;b0&quot;], freq = FALSE, main = &quot;Posterior dist. b0&quot;, xlab = &quot;b0&quot;) abline(v = mean(draws[, &quot;b0&quot;]), lwd = 2, col = &quot;red&quot;) hist(draws[, &quot;b1&quot;], freq = FALSE, main = &quot;Posterior dist. b1&quot;, xlab = &quot;b1&quot;) abline(v = mean(draws[, &quot;b1&quot;]), lwd = 2, col = &quot;red&quot;) hist(draws[, &quot;theta&quot;], freq = FALSE, main = &quot;Posterior dist. theta&quot;, xlab = &quot;theta&quot;) abline(v = mean(draws[, &quot;theta&quot;]), lwd = 2, col = &quot;red&quot;) par(mfrow = c(1, 1)) Code theta_CrI &lt;- quantile(draws[, &quot;theta&quot;], c(0.025, 0.975)) plot(df_a$Ndfa, df_a$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) lines(df_a$Ndfa, pred_mean, col = &quot;red&quot;, lwd = 2) abline(v = mean(draws[, &quot;theta&quot;]), lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = theta_CrI[1], lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = theta_CrI[2], lwd = 1, lty = 2, col = &quot;green&quot;) What changes? Method b0 b1 theta theta_l theta_u LS -78.38 1.35 58.27 ML -78.38 1.35 58.27 52.88 63.65 Bay -78.10 1.34 58.26 52.16 64.2 Why? Simple model Lot’s of observations - No deficiency on informational content Why Bayesian might be useful? Code df_b &lt;- df[which(df$Scenario == &quot;Scenario B&quot;),] plot(df_b$Ndfa, df_b$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;, ylim = c(-100, 100), xlim = c(0, 110)) abline(a = 0, b = 0, lwd = 3, col = &quot;gold&quot;) Considerably less observations and balance (\\(\\theta\\)) is never observed. Using Likelihood: Code # Fit the model m4 &lt;- gls(PartNbalance ~ Ndfa, data = df_b, method = &quot;ML&quot;) # Ndfa to achieve neutral N balance b0hat_ml2 &lt;- as.numeric(coef(m4)[1]) # extract the intercept b1hat_ml2 &lt;- as.numeric(coef(m4)[2]) # extract the slope thetahat_ml2 &lt;- -b0hat_ml2/b1hat_ml2 # calculate theta theta_se2 &lt;- deltamethod(~ -x1/x2, mean = coef(m4), cov = vcov(m4)) theta_ci2 &lt;- c(thetahat_ml2-1.96*theta_se2, thetahat_ml2+1.96*theta_se2) plot(df_b$Ndfa, df_b$PartNbalance, xlim = c(0, 110), ylim = c(-100, 100), xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N Balance (kg/ha)&quot;) abline(a = 0, b = 0, lwd = 3, col = &quot;gold&quot;) abline(m4, lwd = 3, col = &quot;red&quot;) abline(v = thetahat_ml2, lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = theta_ci2[1], lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = theta_ci2[2], lwd = 1, lty = 2, col = &quot;green&quot;) Using Bayesian: Code set.seed(2026) y &lt;- df_b$PartNbalance x &lt;- (df_b$Ndfa)/100 # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;N&quot; = nrow(df_b)) initials &lt;- list( list(&quot;theta&quot; = 0.1, &quot;b1&quot; = 0.1, &quot;sigma&quot; = 0.1) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dnorm(mu[i], tau) mu[i] &lt;- -b1*theta + b1*(x[i]) } # Priors b1 ~ dgamma(1.6, 0.8) theta ~ dbeta(62.52, 14.06) sigma ~ dgamma(2.5, 0.05) # Derived quantity b0 &lt;- -b1*theta tau &lt;- 1/(sigma^2) }&quot; m5 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m5, n.iter = 1000) samples2 &lt;- coda.samples(m5, variable.names = c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;theta&quot;, &quot;sigma&quot;), n.iter = 1000) #summary(samples2) #plot(samples2) # Extract draws and calculate expected line draws2 &lt;- as.matrix(samples2) theta_hat2 &lt;- mean(draws2[, &quot;theta&quot;]) b1_hat2 &lt;- mean(draws2[, &quot;b1&quot;]) pred_mean2 &lt;- b1_hat2* + b1_hat2*x Code theta_CrI2 &lt;- quantile(draws2[, &quot;theta&quot;], c(0.025, 0.975)) plot(df_b$Ndfa, df_b$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;, ylim = c(-100, 100), xlim = c(0, 110)) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) #lines(df_b$Ndfa, pred_mean2, col = &quot;red&quot;, lwd = 2) abline(v = (mean(draws2[, &quot;theta&quot;]))*100, lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = (theta_CrI2[1])*100, lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = (theta_CrI2[2])*100, lwd = 1, lty = 2, col = &quot;green&quot;) Why? Deficient information about \\(\\theta\\) in this dataset. Bayesian offers the opportunity to utilize prior information to constraint \\(\\hat{\\theta}\\). We treat \\(\\theta\\) as a random variable. Highlight from Section 2: There are different approaches to estimate the model parameters, some, rely more heavily on data (Loss function and Likelihood), and some less (Bayesian). Estimation can interfere with inference when few observations are available or low information regarding a specific parameter. Loss function approaches do not allow inference. Inference comes with a price, assumptions! More assumptions, more difficult to work with the model. Choose your battles, it is not always worth to go this route! Questions Sample code is provided for some questions. Code used on the class examples can also be used to work on this practice. Recommended - Based for Day 2 On Days 2 and 3 we will work with an example of logistic growth curve. Growth curves are widely used in biology to study population dynamics. A good reading: Analysis of logistic growth models - Tsoularis and Wallace (2002). Here we will apply it to plant disease growth on time. We will use this model: \\[ y_i = \\frac{K}{1 + (\\frac{K - y_0}{y_0})e^{-rt_i}} + \\varepsilon_i \\] In which the parameters have meaning: \\(K\\) - Maximum disease potential - Maximum severity level \\(r\\) - Disease rate of growth - How fast it grows \\(y_0\\) - Initial disease level The disease data is in the file “nonlin.csv”. In this data, \\(y_i\\) represent disease severity (column: severity, % of leaf area affected by the disease). Our goal is to model severity by time (column = time, days). What else is in this data? Year (column: year) - Two different seasons, 2023 with less disease and 2024 with more Variety (column: var) - A moderately resistant variety (b) and a susceptible one (a) Code # Data nl_df &lt;- read.csv(&quot;nonlin.csv&quot;) nl_24 &lt;- nl_df[which(nl_df$year == 2024 &amp; nl_df$var == &quot;a&quot;),] nl_24$p_sev &lt;- nl_24$severity/100 nl_23 &lt;- nl_df[which(nl_df$year == 2023 &amp; nl_df$var == &quot;a&quot;),] nl_23$p_sev &lt;- nl_23$severity/100 Recall that to work with non-linear models, we either linearize it or fit as it is. First, use linearization to fit a linear model for 2024. We will use a logit transformation: \\[ logit(y_i) = \\beta_0 + \\beta_1*x_i + \\varepsilon_i \\] Where \\(\\beta_1 \\sim r\\) from the nonlinear form. Code # Example for 2024 # For this, we assume K = 1 nl_24$p_sev &lt;- nl_24$p_sev + 0.001 # Correction for log nl_24$l_sev &lt;- log(nl_24$p_sev/(1-nl_24$p_sev)) # Fit the linear model to the logit transformed y m1 &lt;- lm(l_sev ~ time, data = nl_24) # Make predictions logit_pred &lt;- data.frame( time = seq(min(nl_24$time), max(nl_24$time), 1)) logit_pred$pred &lt;- predict(m1, newdata = logit_pred) # Plot predictions plot(nl_24$time, nl_24$l_sev, ylab = &quot;Logit(y)&quot;, xlab = &quot;Days&quot;, ylim = c(-7, 5)) lines(logit_pred$time, logit_pred$pred, lwd = 3, col = &quot;red&quot;) Now, fit the non-linear model for 2024 and 2023. Plot data and predictions. Save models for the next question. Code # Example for 2024 # Fit the logistic model m2 &lt;- nls(p_sev ~ K/(1 + ((K-y0)/y0)*exp(-r*time)), data = nl_24, start = list(K = 0.7, y0 = 0.001, r = 0.2)) # Predict using the logistic curve nl_pred &lt;- data.frame( time = seq(min(nl_24$time), max(nl_24$time), 1)) nl_pred$pred &lt;- predict(m2, newdata = nl_pred) # Plot predictions plot(nl_24$time, nl_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(nl_pred$time, nl_pred$pred, lwd = 3, col = &quot;red&quot;) Extract model parameters. Check the Wald Confidence Intervals. Do you think these values make sense? Code summary(m2) # Overall model summary # Example - For K: ## Extract the estimated value K &lt;- as.numeric(coef(m2)[1]) ## Calculate Wald Confidence Interval for the parameter estimate K_sd &lt;- sqrt(vcov(m2)[1]) K_lb &lt;- K-1.96*K_sd # CI Lower K_ub &lt;- K+1.96*K_sd # CI Upper # Notes for you: ## nls in R uses least squares, a loss function approach. Recall: Minimize the loss function. ## To get CI here we are making additional assumptions that we do not make for loss function approach: iid normal residuals. ## This is possible because LS = ML under iid Normal errors with constant variance. ## Why 1.96? It represents the 97.5th percentile of the std N distribution. Try fitting a polynomial to this data. Does the predicted curve make sense visually? Code # Example for 2024 # Fit the model m3 &lt;- lm(p_sev ~ poly(time, degree = 3), data = nl_24) # Make predictions poly_pred &lt;- data.frame( time = seq(min(nl_24$time), max(nl_24$time), 1)) poly_pred$pred &lt;- predict(m3, newdata = poly_pred) # Plot predictions plot(nl_24$time, nl_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(poly_pred$time, poly_pred$pred, lwd = 3, col = &quot;red&quot;) Are the parameters still carrying biological meaning? How fast is the disease growing? Code summary(m3) Optional - For practice only! Using the field pea and white lupine data, experiment fitting the simple intercept-slope linear model (\\(y_i = \\beta_0 + \\beta_1*xi + \\varepsilon_i\\)) using the likelihood approach. Code # Data ## Field pea data (Scenario A) fp &lt;- df[which(df$Scenario == &quot;Scenario A&quot;),] ## White lupine data (Scenario B) wl &lt;- df[which(df$Scenario == &quot;Scenario B&quot;),] Calculate the derived quantity \\(\\hat{\\theta}\\), and use delta method and bootstrap to obtain confidence intervals. Compare the intervals between the two datasets. Bootstrap is another option to get predictive intervals, instead of using the delta method. Code # Bootstrap for field pea (fp) library(nlme) # gls function for model fit set.seed(2026) # As we are working with a random (stochastic) process, we need to set seed to be able to reproduce the results ite &lt;- 1000 # How many iterations (repetitions of the process) we will have n &lt;- nrow(fp) # Sample size to be collected (= df size) theta.save &lt;- seq(0, ite-1) # Save theta calculated for each sample # Custom bootstrap algorithm for(i in 1:ite){ sample &lt;- fp[sample(nrow(fp), n, replace = TRUE), ] fit &lt;- gls(PartNbalance ~ Ndfa, data = sample, method = &quot;ML&quot;) theta &lt;- -coef(fit)[1]/coef(fit)[2] theta.save[i] &lt;- theta } # Plot the bootstrap distribution hist(theta.save, freq = FALSE, main = &quot;Bootstrap distribution of theta&quot;, xlab = &quot;theta&quot;) # 95% CI quantile(theta.save, probs = c(0.025, 0.975)) Steps: Sample n elements with replacement from the original data For every sample calculate the desired statistic (here \\(\\theta\\)) Repeat steps 1 and 2 n times (iterations = ite) and save the calculated statistic Plot the calculated statistics which forms the bootstrap distribution Using the bootstrap distribution of desired statistics calculate the 95% CI We are using scalar notation throughout this class.↩︎ Base function is a known function of the predictor (\\(x\\)) that gets its own coefficient in a linear model. In short, a mathematical transformation of the predictor. For example, the linear model \\(y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\\) can be written as \\(y_i = \\beta_0 * 1 + \\beta_1 * x + \\varepsilon_i\\), in which the base functions (\\(b\\)) are \\(b_0(x) = 1\\) and \\(b_1(x) = x\\). For polynomial regression, for example, the base function could be represented as \\(b_d(x_i) = x_i^d\\), with \\(d\\) being the polynomial degree.↩︎ Derived quantities are functions of model parameters↩︎ The Delta Method approximates the uncertainty of a transformed estimator (derived quantity), assuming it was originally approximately normal.↩︎ "],["welcome-to-day-2.html", "Welcome to Day 2!! Nonlinear models", " Welcome to Day 2!! Yesterday we: Discussed what is a linear model and what is not - Linear on the parameters. We showed that linear models \\(\\neq\\) lines. We showed that linear models \\(\\neq\\) simplicity. Discussed different estimation approaches. Loss function approach - Minimize the residual sum (distance between obs and pred.). Likelihood-based approach - Maximize likelihood = Values of parameters that maximize the probability of the observed data under the model we assumed. Bayesian approach - Prior knowledge + data = Values the parameter can assume with different probability - Whole distribution (posterior distribution). Discussed briefly when they are equal (e.g., independent and individualy distributied normal errors) and when they are not (violate this assumption). Nonlinear models Outline: Section 3: Exercises from day 1 Linearization - Pros and cons Mechanistic vs Phenomenological models Identifiability of parameters Section 4: Working with non-identifiability Fixing weakly identifiable parameters Extra: Summary Questions Recommended - Base for Day 3 Optional - For practice only! What we expect you to learn on Day 2: Linearization is useful and common, but it is heavy in assumptions that might not fit your research question. Nonlinear models are useful, but sometimes linear models can fit perfectly depending on your research question/goals. Identifiability is a big thing when using nonlinear models, we need to know what it is and pay attention on how to remedy it. There is no free lunch, every method has assumptions, and assumptions can be expensive! Section 3: Exercises from day 1 You were asked to work with this data: What is it really about? Two years/seasons: 2023 (less disease) and 2024 (more disease) Two varieties: a -&gt; Susceptible b -&gt; Moderately resistant Linearized logistic model for 2024, variety a Code # Data 2024 variety a a_24 &lt;- df_a[which(df_a$year == 2024),] a_24$p_sev &lt;- a_24$p_sev + 0.001 # Logit transformation a_24$l_sev &lt;- log(a_24$p_sev/(1-a_24$p_sev)) # Logit # Fit the model m1 &lt;- lm(l_sev ~ time, data = a_24) # Make predictions m1_pred &lt;- data.frame( time = seq(min(a_24$time), max(a_24$time), by = 1) ) m1_pred$pred &lt;- predict(m1, newdata = m1_pred) # Plot predictions m1_pred$inv_logit &lt;- 1/(1+exp(-m1_pred$pred)) plot(a_24$time, a_24$l_sev, ylab = &quot;Logit(y)&quot;, xlab = &quot;Days&quot;, ylim = c(-7, 5)) lines(m1_pred$time, m1_pred$pred, lwd = 3, col = &quot;red&quot;) Code plot(a_24$time, a_24$p_sev, ylab = &quot;y_prop.&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1.1)) lines(m1_pred$time, m1_pred$inv_logit, lwd = 3, col = &quot;gold&quot;) What is actually going on here? Logit: \\[ logit(y) = log(\\frac{y}{K-y}) \\] We have to assume a value for \\(K\\), “removing” this parameter. Our intercept (\\(\\beta_0\\)) it not as simple as before: \\[ \\beta_0 = log(\\frac{y_0}{K-y_0}) \\] We also “remove” \\(y_0\\). We still have our slope \\(\\beta_1\\), and we can use it to make inference about the disease growth, but it is not the same as \\(r\\). Overall: \\(\\beta_1\\) describes the change in \\(\\mathbf{y}\\) with one unit change in \\(\\mathbf{x}\\) \\(r\\) describes how quickly the epidemic accelerates and approaches its upper bound If you want more details: \\(\\beta_1\\): Multiply \\(x_i\\) and is additive, each unit change in \\(\\mathbf{x}\\) adds \\(\\beta_1\\) to \\(\\mathbf{y}\\). Shifts the mean linearly. Effect is independent on \\(\\mathbf{y}\\) value. No bounds. No direct biological meaning. Easier to estimate with low correlation to \\(\\beta_0\\). \\(r\\) Multiply \\(x_i\\), but is on an exponential, each unit change in \\(\\mathbf{x}\\) is multiplicative in the growth (differential equation5). Affects shape and trajectory of the entire curve. Effect changes with \\(\\mathbf{y}\\) value. Bounded between 0 and \\(K\\) Direct biological meaning. Estimation sensitive to information on the data, often correlated with \\(K\\). The actual linearized model: \\[ log(\\frac{y_i}{1 - y_i}) = log(\\frac{y_0}{1 - y_0}) + \\beta_1*x_i + \\varepsilon_i \\] Pros: Simple to use; require less information from the data collected to fit. Cons: No inference on important parameters such as \\(K\\) and \\(y_0\\); more complex to make sense of the response variable \\(logit(severity)\\); sensibility to extreme values (close to 0 or 100); \\(\\beta_1\\) and \\(r\\) do not have the same meaning (\\(\\beta_1*x_i\\) adds, while \\(r*x_i\\) is exponential). Also, more assumptions = More bias! To be discussed on day 3! Mechanistic vs Phenomenological models - Logistic and polynomial models for 2024 variety a Code # Fit the logistic model m2 &lt;- nls(p_sev ~ K/(1 + ((K-y0)/y0)*exp(-r*time)), data = a_24, start = list(K = 0.7, y0 = 0.001, r = 0.2)) # Make predictions m2_pred &lt;- data.frame( time = seq(min(a_24$time), max(a_24$time), by = 1)) m2_pred$pred &lt;- predict(m2, newdata = m2_pred) Code # Fit the polynomial model m3 &lt;- lm(p_sev ~ poly(time, degree = 3), data = a_24) # Make predictions m3_pred &lt;- data.frame( time = seq(min(a_24$time), max(a_24$time), by = 1)) m3_pred$pred &lt;- predict(m3, newdata = m3_pred) Code par(mfrow = c(1, 2)) # Plot predictions for logistic plot(a_24$time, a_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1), main = &quot;Logistic&quot;) lines(m2_pred$time, m2_pred$pred, lwd = 3, col = &quot;red&quot;) # Plot predictions for polynomial plot(a_24$time, a_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1), main = &quot;Polynomial&quot;) lines(m3_pred$time, m3_pred$pred, lwd = 3, col = &quot;red&quot;) Code par(mfrow = c(1, 1)) What is the difference? Why would we choose one or the other? Board What is the disease rate of growth? Identifiability of parameters - Logistic model for variety a in 2023 and 2024 What is identifiability? In general terms, when a parameter is weakly constrained ~ We are very uncertain about that parameter Two types: Structural: This is caused by the model structure Practical: Caused by the data - Limited data lack information on a specific parameter Board What it is - Example Why is it very important in nonlinear models To understand what is causes, look at the parameters of our model for both years with Wald confidence intervals6: High uncertainty around parameter estimates! - Classical symptom of weak identifiability Another classical symptom is correlation between parameters. How does it look like for our models? 2023 correlation matrix ## K y0 r ## K 1.0000000 0.9761482 -0.9771736 ## y0 0.9761482 1.0000000 -0.9999097 ## r -0.9771736 -0.9999097 1.0000000 2024 correlation matrix ## K y0 r ## K 1.0000000 0.6486111 -0.6869020 ## y0 0.6486111 1.0000000 -0.9946642 ## r -0.6869020 -0.9946642 1.0000000 Why it happens? The data inform combinations of parameters instead of individual parameters It created dependencies - Dependencies between r and K in this case. Recall how our data looks like: Section 3: Exercises from day 1 Highlight from Section 3: Linearization is an option, but it is heavy in underlying assumptions and works on a transformed scale. Depending on your goal (prediction [phenomenological route] or parameter inference [mechanistic route]), more complex linear models might be enough, and you not necessarily require a nonlinear model. Weak identifiability, in general terms, happens when your data has low information about an specific parameters. Weak identifiability is an important and recurrent topic in nonlinear models, and your can remedy (but note “resolve”) it with different strategies. These strategies require more assumptions. Assumptions introduce bias, bias reduce variance, this is why assumptions need to be deeply considered, they are should not be lightly taken. Section 4: Working with weakly identifiable parameters What are our options? Adjust the experimental design and/or data collection method Collect more data associated with the weakly identified parameters Reparametrize weakly identified parameters Simplify the model (e.g. fixing weakly identified parameters) Linearization also simplifies the model, and \\(\\beta_1\\) is rarely weakly identifiable Impose constrains on these parameters using prior knowledge A good paper about identifiability: Preston et al. 2025 Fixing weakly identifiable parameters - Variety a in 2023 Recall 2023 disease data for progress in time: If we are very interested in \\(r\\), so we can understand how fast is growth, we can fixate other parameters. Let’s assume \\(K = 1\\): \\[ y_i = \\frac{1}{1+(\\frac{1 - y_0}{y_0})*e^{-rx_i + \\varepsilon_i}} \\] Code # Data variety a, 2023 a_23 &lt;- df[which(df$var == &quot;a&quot; &amp; df$year == 2023),] # Fit the model m4 &lt;- nls(p_sev ~ 100/(1 + ((100-y0)/y0)*exp(-r*time)), data = a_23, start = list(y0 = 0.001, r = 0.2)) # Make predictions m4_pred &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) m4_pred$pred &lt;- predict(m4, newdata = m4_pred) # Plot predictions plot(a_23$time, a_23$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(m4_pred$time, m4_pred$pred, lwd = 3, col = &#39;red&#39;) How it compares to estimating \\(K\\)? How about the parameters? The data does not inform \\(K\\) when we fix it, we inform it! Highlight from Section 4: Some assumptions are very strong, such as \\(K = 1\\), but depending on your research question, they might make sense. All assumptions have a cost, being worth this cost depends on your objectives with the analysis. Extra: Summary Today we discussed: Linearization Mechanistic vs Phenomenological models Identifiability What it is How to work with it - Fixing parameters and simplification Which models did we use in this day? Logistic Disease Progress Curve (nonlinear, mechanistic): \\[ y_i = \\frac{K}{1 + (\\frac{K - y_0}{y_0})e^{-r*x_i + \\varepsilon_i}} \\] Logit-transformed linear model (linear, phenomenological): \\[ log(\\frac{y_i}{1 - y_i}) = log(\\frac{y_0}{1 - y_0}) + \\beta_1*x_i + \\varepsilon_i \\] Which assumption we made? For all models: \\[ \\varepsilon_i \\sim i.i.d. N(0, \\sigma^2) \\] For the linearized and simplified we also did: \\[ K = 1 \\] *Note on nls function in R: It uses Least Squares, a loss function approach, however, it does make the assumption \\(\\varepsilon_i \\sim i.i.d.N(0, \\sigma^2)\\), which gives us inference. Under this assumption [that errors are normal, independent, and individually distributed]: Likelihood approach = Loss function approach. We have seen that in Day 1! Questions Sample code is provided for some questions. Code used on the class examples can also be used to work on this practice. Code # Data df &lt;- read.csv(&quot;nonlin.csv&quot;) df$p_sev &lt;- df$severity/100 Recommended - Base for day 3 For 2023, fit the nonlinear logistic model to variety a and b. Extract \\(K\\) and \\(r\\) and check how genetic resistance affect these parameters. Code # Let&#39;s fit the model for each variety individually # Data a_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),] b_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;b&quot;),] # Fit the models ma &lt;- nls(p_sev ~ K/(1+((K-y0)/y0)*exp(-r*time)), data = a_23, start = c(K = 0.7, y0 = 0.0001, r = 0.2)) mb &lt;- nls(p_sev ~ K/(1+((K-y0)/y0)*exp(-r*time)), data = b_23, start = c(K = 0.7, y0 = 0.0001, r = 0.2)) # Make predictions pred_df &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) pred_df$pred_a &lt;- predict(ma, newdata = pred_df) pred_df$pred_b &lt;- predict(mb, newdata = pred_df) # Plot predictions plot(a_23$time, a_23$p_sev, type = &quot;p&quot;, col = &quot;darkred&quot;, ylim = c(0, 1), ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;) points(b_23$time, b_23$p_sev, col = &quot;orange&quot;) lines(pred_df$time, pred_df$pred_a, lwd = 3, col = &#39;red&#39;) lines(pred_df$time, pred_df$pred_b, lwd = 3, col = &#39;gold&#39;) # Extract parameters ## K K_a &lt;- coef(ma)[1] K_a K_b &lt;- coef(mb)[1] K_b ## r r_a &lt;- coef(ma)[3] r_a r_b &lt;- coef(mb)[3] r_b Repeat the previous question, but now, for 2024. Optional - For practice only! Write down the model you are using on the first question and which assumption you are making. Repeat what you did in the first question, but assume \\(K = 1\\). Can you see differences? Code # Let&#39;s fit the model for each variety individually # Data a_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),] b_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;b&quot;),] # Fit the models ma &lt;- nls(p_sev ~ 1/(1+((1-y0)/y0)*exp(-r*time)), data = a_23, start = c(y0 = 0.0001, r = 0.2)) mb &lt;- nls(p_sev ~ 1/(1+((1-y0)/y0)*exp(-r*time)), data = b_23, start = c(y0 = 0.0001, r = 0.2)) # Make predictions pred_df &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) pred_df$pred_a &lt;- predict(ma, newdata = pred_df) pred_df$pred_b &lt;- predict(mb, newdata = pred_df) # Plot predictions plot(a_23$time, a_23$p_sev, type = &quot;p&quot;, col = &quot;darkred&quot;, ylim = c(0, 1), ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;) points(b_23$time, b_23$p_sev, col = &quot;orange&quot;) lines(pred_df$time, pred_df$pred_a, lwd = 3, col = &#39;red&#39;) lines(pred_df$time, pred_df$pred_b, lwd = 3, col = &#39;gold&#39;) # Extract parameters ## r r_a &lt;- coef(ma)[2] r_a r_b &lt;- coef(mb)[2] r_b Use the logit transformation on the data and fit a linear model. Compare the disease progress in varieties a and b. Code # Data a_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),] a_23$p_sev &lt;- a_23$p_sev + 0.001 a_23$l_sev &lt;- log(a_23$p_sev/(1-a_23$p_sev)) b_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;b&quot;),] b_23$p_sev &lt;- b_23$p_sev + 0.001 b_23$l_sev &lt;- log(b_23$p_sev/(1-b_23$p_sev)) # Fit the models ma &lt;- lm(l_sev ~ time, data = a_23) mb &lt;- lm(l_sev ~ time, data = b_23) # Make predictions pred_df &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) pred_df$pred_a &lt;- predict(ma, newdata = pred_df) pred_df$pred_b &lt;- predict(mb, newdata = pred_df) # Plot predictions plot(a_23$time, a_23$l_sev, type = &quot;p&quot;, col = &quot;darkred&quot;, ylab = &quot;Logit(y)&quot;, xlab = &quot;Days&quot;) points(b_23$time, b_23$l_sev, col = &quot;orange&quot;) lines(pred_df$time, pred_df$pred_a, lwd = 3, col = &#39;red&#39;) lines(pred_df$time, pred_df$pred_b, lwd = 3, col = &#39;gold&#39;) # Extract parameters ## b1 b1_a &lt;- coef(ma)[2] b1_a b1_b &lt;- coef(mb)[2] b1_b Write down the model you are using in the previous questions and which assumptions are you making. \\(\\frac{dy}{dt} = \\mathbf{ry}(1-\\frac{\\mathbf{y}}{\\mathbf{K}})\\)↩︎ Wald CI measure local uncertainty (around the parameter estimate) and not at the whole parameter space (all values it could assume - global), therefore, they might be more optimistic than global CI. Bootstrap or Bayesian approach would provide more closely a global uncertainty measure once they explore a broader region of the parameter space.↩︎ "],["welcome-to-day-3.html", "Welcome to day 3!!! Bayesian Nonlinear models", " Welcome to day 3!!! What have we learned so far? Day 1 What is and what isn’t a linear model. Estimation approaches: Loss function; Likelihood-based; Bayesian. When they are similar (normal errors). Day 2 Linearization -&gt; From a curve, to a line… to a curve. Mechanistic and Phenomenological models -&gt; Describe what causes patterns vs Describe the patterns. Practical identifiability of parameters -&gt; When our data cannot inform our model parameters. How to work with weak identifiability. Bayesian Nonlinear models Outline: Section 5: Exercises from day 2 Model selection, why? Section 6: Bayesian nonlinear models Bayesian logistic disease progress curve Why Bayesian might be helpful? Wrap-up What we expect you to learn on Day 3: We have different approaches for choosing the mathematical model, however, once these models have biological meaning, this should be the most important to consider. Bayesian models expand what we did before by adding more assumptions. The way we write Bayesian models make these assumption explicit. A prior distribution is built based on what we already know (prior knowledge), and it helps constraining our model parameters. Constraining the parameters with prior knowledge helps to remedy weak identifiability. Section 5: Exercises from day 2 Why logistic? For 2023, fit the nonlinear logistic model to variety a and b. Extract \\(K\\) and \\(r\\) and check how genetic resistance affect these parameters. Variety K r a 0.6684032 0.3361402 b 0.5085910 0.3623941 Repeat the previous question, but now, for 2024. How did it go? More specifically, where you able to fit it for variety b? What we have: What we expect: The blue line represents an exponential mathematical model: \\[ y_i = y_0*e^{r_e*x_i} \\] Why don’t we use this model instead? I have been pushing the logistic model for this data, even if it might not be the one that “best fit” it. Why? Discussion Highlight from Section 5: Different techniques can help with model selection, however, for mechanistic models that have deep biological meaning, this criteria should be the first to be considered. Section 6: Bayesian nonlinear models We need more assumptions, one of our parameters is non-identifiable. A parallel with day 2: When data was insufficient, we made assumptions: \\(K = 1\\) + Nonlinear logistic model. \\(K = 1\\) + Linearization. Assume the nonlinear relationship is well approximated by a linear one. Assume inference on linearized model parameters is equivalent to the nonlinear. Mechanistic meaning of the nonlinear model is preserved. *Bayesian makes assumptions explicit vs previous approaches. What have we been doing so far with the logistic model (now depicted by distributional notation)? \\[ y_i \\sim N(\\mu_i, \\sigma^2) \\\\[10pt] \\mu_i = \\frac{K}{1+(\\frac{K-y_0}{y_0})*e^{-r*x_i}} \\] This is not Bayesian. This is not working because of weak identifiability. But we want this logistic equation, so let’s make more assumptions! Bayesian logistic disease progress curve The first assumption we can make regards our response variable! “If you are already in the rain, do not fear getting wet!” Is our model matching the data generating process? Severity = Proportion -&gt; Bounded 0 - 1 Beta distribution (Beta regression - Ferrari &amp; Cribari-Neto, 2010). \\[ y_i \\sim Beta(\\mu_i, \\phi) \\\\[10pt] \\mu_i = \\frac{K}{1+(\\frac{K-y_0}{y_0})*e^{-r*x_i}} \\] This still not Bayesian. We are just choosing a more appropriate distribution for our response variable. We can do that using likelihood-based estimation. Now, let’s get Bayesian! We will treat our unknown quantities (parameters) as random variables. How does it work? Recall the assumption: \\(K = 1\\) It is the same as: \\(K \\sim N(1, 0)\\) This is the strongest and most restrictive assumption! If we “relax” this assumption, and let \\(K\\) vary a little more, let’s say \\(K \\sim N(1, 5)\\), this what happens: Now, do you think these values make sense? \\(K\\) has a biological meaning - Maximum disease potential. What if we impose a constraint on this parameter? \\[ K \\sim Beta(6, 2) \\] How we are choosing that? Beta distribution - Bounds - Support helps a lot! Leaning towards higher values - \\(K\\) represents the maximum disease potential. Let it vary from very small maximum to the maximum possible (1). This is what we call prior distribution, we use prior knowledge to inform unknown quantities in the model. “Before we see the data, what values we expect this parameter to assume?” Priors are constructed using: Expert knowledge. Published information. Simulation/previous studies. We end up with this Bayesian model: \\[ y_i \\sim Beta(\\mu_i, \\phi) \\\\[10pt] \\mu_i = \\frac{K}{1 + (\\frac{K - y_0}{y_0})*e^{-r*x_i}} \\\\[20pt] K \\sim Beta(6, 2) \\\\[10pt] y_0 \\sim Unif(0.001, 0.1) \\\\[10pt] r \\sim Unif(0.1, 0.5) \\\\[10pt] \\phi \\sim Gamma(24, 2) \\] Let’s fit the model for the variety b in 2024: Code library(rjags) library(coda) set.seed(2026) y &lt;- df[which(df$year == 2024 &amp; df$var == &quot;b&quot;),]$p_sev + 0.001 x &lt;- df[which(df$year == 2024 &amp; df$var == &quot;b&quot;),]$time N &lt;- nrow(df[which(df$year == 2024 &amp; df$var == &quot;b&quot;),]) x.pred &lt;- seq(1, max(x)+20, by = 1) N_pred &lt;- length(x.pred) # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;x.pred&quot; = x.pred, &quot;N&quot; = N, &quot;N_pred&quot; = N_pred) initials &lt;- list( list(&quot;K&quot; = 0.7, &quot;r&quot; = 0.2, &quot;y0&quot; = 0.001, &quot;phi&quot; = 10) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dbeta(alpha[i], beta[i]) alpha[i] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x[i])))*phi beta[i] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x[i]))))*phi } # Posterior pred. for(z in 1:N_pred){ y_pred[z] ~ dbeta(alpha_pred[z], beta_pred[z]) alpha_pred[z] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z])))*phi beta_pred[z] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z]))))*phi } # Priors y0 ~ dunif(0.001, 0.1) K ~ dbeta(6, 2) r ~ dunif(0.1, 0.5) phi ~ dgamma(24, 2) }&quot; m1 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m1, n.iter = 10000) samples &lt;- coda.samples(m1, variable.names = c(&quot;K&quot;, &quot;y0&quot;, &quot;r&quot;, &quot;phi&quot;, &quot;y_pred&quot;), n.iter = 10000) # Extract draws ## Parameters par_post &lt;- as.matrix(samples[,c(1, 2, 3, 4)]) ## Predictions pred_post &lt;- as.matrix(samples[,c(5:61)]) How the predictions (expected value) looks like with 95% Posterior Predictive Interval7: Code # Quantiles for 95% Credible Interval quant &lt;- apply(pred_post, 2, quantile, probs = c(0.025, 0.975)) # DF for prediction bay_pred &lt;- data.frame( y.pred = colMeans(pred_post), y.lb = quant[1,], y.ub = quant[2,], x.pred = x.pred ) # Plot prediction plot(bay_pred$x.pred, bay_pred$y.pred, type = &quot;l&quot;, ylab = &quot;Pred. severity (mean)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1.1), lwd = 3, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.lb, lty = 2, lwd = 3, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.ub, lty = 2, lwd = 3, col = &#39;red&#39;) abline(h = 1, lty = 3, col = &#39;black&#39;, lwd = 2) abline(h = 0, lty = 3, col = &#39;black&#39;, lwd = 2) How about the parameters? Code par(mfrow = c(2, 2)) # For K hist(par_post[,1], ylab = &quot;[K]&quot;, xlab = &quot;K&quot;, main = &quot;Post. dist. K&quot;, freq = FALSE) abline(v = mean(par_post[,1]), lty = 2, lwd = 3, col = &#39;red&#39;) # For r hist(par_post[,3], ylab = &quot;[r]&quot;, xlab = &quot;r&quot;, main = &quot;Post. dist. r&quot;, freq = FALSE) abline(v = mean(par_post[,3]), lty = 2, lwd = 3, col = &#39;red&#39;) # For y0 hist(par_post[,4], ylab = &quot;[y0]&quot;, xlab = &quot;y0&quot;, , main = &quot;Post. dist. y0&quot;, freq = FALSE) abline(v = mean(par_post[,4]), lty = 2, lwd = 3, col = &#39;red&#39;) par(mfrow = c(1, 1)) How posterior compare to prior? Let’s look at \\(r\\). Recall: \\(r \\sim Unif(0.1, 0.5)\\) Code # Post. dist. r hist(runif(1000,0.1, 0.5), freq = FALSE, main = &quot;Prior dist. r&quot;, ylab = &quot;[r]&quot;, xlab = &quot;r&quot;, xlim = c(0.1, 0.5)) Code # Pior dist. r hist(par_post[,3], ylab = &quot;[r]&quot;, xlab = &quot;r&quot;, main = &quot;Posterior dist. r&quot;, freq = FALSE, xlim = c(0.1, 0.5)) Why Bayesian might be helpful? Let’s use variety a in 2023 for and example. Code #### Fit the Bayesian model for 2024 var a #### set.seed(2026) library(rjags) y &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),]$p_sev + 0.001 x &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),]$time N &lt;- nrow(df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),]) x.pred &lt;- seq(1, max(x)+20, by = 1) N_pred &lt;- length(x.pred) # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;x.pred&quot; = x.pred, &quot;N&quot; = N, &quot;N_pred&quot; = N_pred) initials &lt;- list( list(&quot;K&quot; = 0.7, &quot;r&quot; = 0.2, &quot;y0&quot; = 0.001, &quot;phi&quot; = 10) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dbeta(alpha[i], beta[i]) alpha[i] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x[i])))*phi beta[i] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x[i]))))*phi } # Posterior pred. for(z in 1:N_pred){ y_pred[z] ~ dbeta(alpha_pred[z], beta_pred[z]) alpha_pred[z] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z])))*phi beta_pred[z] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z]))))*phi } # Priors y0 ~ dunif(0.001, 0.1) K ~ dbeta(6, 2) r ~ dunif(0.1, 0.5) phi ~ dgamma(24, 2) }&quot; m2 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m2, n.iter = 10000) samples &lt;- coda.samples(m2, variable.names = c(&quot;K&quot;, &quot;y0&quot;, &quot;r&quot;, &quot;phi&quot;, &quot;y_pred&quot;), n.iter = 10000) # Extract draws ## Parameters par_post &lt;- as.matrix(samples[,c(1, 2, 3, 4)]) ## Predictions pred_post &lt;- as.matrix(samples[, c(5:77)]) Code #### Fit the model using likelihood to 2023 var a #### a_24 &lt;- df[which(df$var == &quot;a&quot; &amp; df$year == 2023),] # Fit the model m3 &lt;- nls(p_sev ~ K/(1+((K-y0)/y0)*exp(-r*time)), start = c(K = 0.7, r = 0.2, y0 = 0.001), data = a_24) # Predictions with prediction interval df_pred &lt;- data.frame( time = seq(1, max(a_24$time)+20, by = 1) ) library(mvtnorm) # Package for multivariate normal set.seed(2026) ite &lt;- 10000 # Number of iterations/simulations par_hat &lt;- coef(m3) # Extract parameter estimated var_cov &lt;- vcov(m3) # Extract var covar matrix sigma_hat &lt;- summary(m3)$sigma # Extract standard deviation par_sim &lt;- rmvnorm(ite, par_hat, var_cov) # Simulate parameters - Draws from a multivariate normal using our expected value for the parameter and their variance time &lt;- df_pred$time sim.save &lt;- matrix(nrow = ite, ncol = length(time)) for(i in 1:ite){ K &lt;- par_sim[i,&quot;K&quot;] y0 &lt;- par_sim[i, &quot;y0&quot;] r &lt;- par_sim[i, &quot;r&quot;] mu &lt;- K/(1+((K-y0)/y0)*exp(-r*time)) y_sim &lt;- rnorm(length(time), mu, sigma_hat) sim.save[i,] &lt;- y_sim } # Prediction + pred intervals on the df df_pred$pred &lt;- predict(m3, newdata = df_pred) df_pred$pred_ub &lt;- apply(sim.save, 2, quantile, 0.975) df_pred$pred_lb &lt;- apply(sim.save, 2, quantile, 0.025) Code # Plot predictions with interval for iid normal assumption plot(a_24$time, a_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, main = &quot;Likelihood approach with minimal assumptions&quot;, ylim = c(-0.5, 1.1)) lines(df_pred$time, df_pred$pred, lwd = 3, col = &#39;red&#39;) lines(df_pred$time, df_pred$pred_ub, lty = 2, col = &#39;red&#39;) lines(df_pred$time, df_pred$pred_lb, lty = 2, col = &#39;red&#39;) Code # Plot predictions with interval for Bayesian model # Quantiles for 95% Credible Interval quant &lt;- apply(pred_post, 2, quantile, probs = c(0.025, 0.975)) # DF for prediction bay_pred &lt;- data.frame( y.pred = colMeans(pred_post), y.lb = quant[1,], y.ub = quant[2,], x.pred = x.pred ) # Plot prediction plot(x, y, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, main = &quot;Bayesian model&quot;, ylim = c(-0.1, 1.1)) lines(bay_pred$x.pred, bay_pred$y.pred, lwd = 3, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.lb, lty = 2, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.ub, lty = 2, col = &#39;red&#39;) How about the parameters? Estimation Parameter Estimate Est_lb Est_ub Min. Assump. K 0.6684032 0.0534314 1.2833750 Min. Assump. r 0.3361402 -1.0979691 1.7702495 Min. Assump. y0 0.0000002 -0.0000130 0.0000134 Bayesian K 0.8366702 0.6189120 0.9744694 Bayesian r 0.1250087 0.1032116 0.1436959 Bayesian y0 0.0019052 0.0010205 0.0043641 Highlight from Section 6: Prior knowledge involves what we already now before even seeing the data. Prior distributions are selected based in prior knowledge. We treat parameters as random variables that arise from probability distributions. A straight forward to check if a distribution make sense for a parameter or the data generating process involves understand it’s support. Bayesian models do not “resolve” weak identifiability, but remedy it by constraining parameters using prior knowledge. In Bayesian models, prior distribution is “updated” by the observed data, resulting in the posterior distribution. Bayesian models are heavy in assumptions, but also very explicit about them. By mindful! Assumptions introduce bias, therefore, they need to be deeply considered. Making more assumptions is adequate to mitigate deficiencies in the data. If there are none, there is no reason not to use the simple. As you learned, it works just the same! Wrap-up There is no right or wrong! There are different assumptions, and each carry advantages and disadvantages. Choosing assumptions should be done consciously, keeping in mind that they might change the analysis results, and how you make inference. Bayesian is not a better approach. It is another tool in your toolbox, and might be helpful in some situations. Before: Less uncertainty = More data Now: Other options -&gt; Less uncertainty = More assumptions All workshop material is available on this page. Feel free to contact us as well if you have questions! Thank you very much for you attention and for attending this workshop, this was a great opportunity for us all to learn more together! Stay tuned for the next workshop “Bayesian Modeling of Designed Experiments” on March 2, 4, and 5. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
