[["index.html", "Non-Linear Models for the Plant Sciences About", " Non-Linear Models for the Plant Sciences Claudio Dias da Silva Jr &amp; Trevor Hefley 2026-02-02 About This material is the base for the workshop Nonlinear Models for the Plant Sciences taught at Kansas State University on the Spring of 2026. This workshop was develop as a collaboration from the Department of Statistics, Department of Plant Pathology and the Institute for Digital Agriculture &amp; Advanced Analytics (ID3A) as part of the Advanced Analytics Workshops series. Motivation: Non-linear models are widely used in plant sciences. Just like linear models, non-linear models can be fitted to data to enable statistical inference, which is required for scientific investigations and can aid when making management decisions. For example, logistic disease progression curves are a non-linear model used to understand plant disease epidemics, and statistical estimation and inference are needed to connect field data to management decisions (e.g., when to spray fungicide). “Non-Linear Models for the Plant Sciences” is designed to help you develop an understanding of non-linear models and how they fit data using common statistical techniques such as maximum likelihood estimation and Bayesian estimation. This workshop is co-hosted by the Departments of Statistics and Plant Pathology. "],["pre-workshop-guide.html", "Pre-workshop guide What will we learn? Notation guide", " Pre-workshop guide What will we learn? Day 1: Linear vs Nonlinear models - What is the difference? Parameter estimation: Loss-function approach Likelihood-based approach Bayesian approach Why Bayesian inference may be useful? Day 2: Linearization - What is going on behind the scenes? Types of models: Mechanistic vs Phenomenological Identifiability of parameters - Deeply related to estimation Working with weakly identifiable parameters using likelihood-based approach Fixing parameters Linearization Day 3: Reasoning behind model selection - Mechanistic models and their meaning A Bayesian example: Bayesian Disease Progress Curve Why use Bayesian? Wrap-up Notation guide We can write the same linear model using different notations, but preserving the same meaning. In this workshop, we will primarily use scalar and probabilistic notation for clarity. Vector and matrix notation are included here for reference. All notations below assume normally distributed errors. \\[ \\varepsilon_i \\sim N(0, \\sigma^2) \\] The Multivariate Normal distribution (MVN) is a compact way to represent multiple normally distributed observations together. Scalar notation - Represents a single number \\[ y_i = \\beta_0 + \\beta_1*x_i + \\varepsilon_i \\\\ \\varepsilon_i \\sim N(0, \\sigma^2) \\] Vector notation - Represents a vector of numbers \\[ \\mathbf{y} = \\beta_0 + \\beta_1*\\mathbf{x}+\\boldsymbol{\\varepsilon} \\\\[10pt] \\begin{bmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n \\end{bmatrix} = \\beta_0 + \\beta_1*\\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ ... \\\\ \\varepsilon_n \\end{bmatrix} \\\\[15pt] \\boldsymbol{\\varepsilon} \\sim MVN(\\mathbf{0}, \\sigma^2\\mathbf{I}) \\] Matrix notation - Represents a matrix \\[ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\\\ \\boldsymbol{\\varepsilon} \\sim MVN(\\mathbf{0}, \\sigma^2\\mathbf{I}) \\] Probabilistic notation - Represents the distribution that we are assuming, a clear way to make our assumptions transparent. The expected value, or mean, can be expressed using any of the previous notations. To follow the same structure of the workshop, scalar notation will be exemplified. In distributional notation, see how we demonstrate that the deterministic mathematical equation influences the expectation/mean. The variance corresponds to the error term previously introduced as \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). This notation reads: Observations \\(y_i\\) arise from a normal distribution with the expectation/mean \\(\\mu_i\\) controlled by our mathematical equation and a given variance \\(\\sigma^2\\). This variance relates to the observational level variance. This notation emphasizes that the model is a statement about how data are generated, combining a deterministic structure with stochastic variability. \\[ y_i \\sim N(\\mu_i, \\sigma^2) \\\\ \\mu_i = \\beta_0 + \\beta_1*x_i \\] Models are assumptions + Structure + Uncertainty! "],["welcome-to-day-1.html", "Welcome to day 1! Linear models", " Welcome to day 1! In this workshop material, theory and R codes are mixed. Throughout the workshop days, our main focus will be on understanding the why. Our main goal is to demonstrate that all codes in this workshop are used after deep understanding of what is going on behind it, after we thought about our models in depth. We will follow a step-wise structure: Day 1: Linear models and parameter estimation Day 2: Nonlinear models and their challenges Day 3: Bayesian nonlinear models More importantly, don’t panic! We will be learning together, slowly. We do not expect you to come into the workshop (or leave it) knowing everything. Our goal is to provide an immersion into the topic and some initial guidance, so you can move forward with your own projects and continued learning. Please feel free to stop me and ask questions at any time. I will answer it to the best of my knowledge, and if I don’t know the answer, either Trevor will jump in or I will make sure to come back to you with a clear explanation in a later moment. Linear models Outline: What is and what isn’t a linear model Estimation and Inference: A linear model example using: Loss function approach Likelihood-based approach Bayesian approach Why use Bayesian Practice questions Fixation day 1 (Optional) Pre-day 2 (Recomended) What is day 1 about? Understanding linear models and estimation conceptually 1. What is and what isn’t a linear model Recap on linear models Recall the famous intercept-and-slope model, written in the “model equation form”1: \\[ y_i = \\beta_0 + x_i\\beta_1 + \\varepsilon_i \\] Here we have the observed value of the \\(i\\)th observation (\\(y_i\\)), the intercept (\\(\\beta_0\\)), representing the level of \\(y\\) when \\(x\\) = 0, the slope (\\(\\beta_1\\)), which for a continuous \\(x_i\\), it represents the change in \\(y\\) given a one unit change in \\(x\\), and the residual (\\(\\varepsilon_i\\)) that represents the different between the observation and the model line for each point. Now, what makes a model linear? A model is linear if it is linear in its parameters. Other examples of linear models are: Polynomial regression: Extend the linear regression in a way that the relationship between the predictors and the response is nonlinear. A polynomial function is a special case of base function2. \\[ y_i = \\beta_0 + \\beta_1x_i+\\beta_2x_i^2+\\varepsilon_i \\] Regression splines: In this type of model, \\(x\\) is partitioned by \\(K\\) points and within each space between these points, a polynomial model is fitted. Splines are a special case of piecewise degree-d polynomial, in which a constraint ensures that it is continuous. This means that at each “knot” (\\(K\\)), that define a breaks in \\(x\\), the line remains continuous. Simple piecewise polynomial regression exist, but they do not ensure this continuity. \\[ y_i = \\beta_0 + \\beta_1b_1(x_i) + \\beta_2b_{K + 2}(x_i) + \\varepsilon_i \\] Yes, this is linear! Linearity is about the geometry of the parameter space, not about the shape of the curve in the data. Shape \\(\\neq\\) Linearity Linear models \\(\\neq\\) Simplicity Simple rule: A model is linear if: All unknown parameter only multiplies known quantities All parameters are added together All parameters are outside nonlinear functions. \\[ \\mathbf{\\hat{y}} = \\sum_i\\beta_ib_i(\\mathbf{x}) \\] What about nonlinear models? In simple terms, in nonlinear models, parameters are inside nonlinear functions. Example: Exponential model: \\[ y_i = \\beta_0*e^{\\beta_1x_i} + \\varepsilon_i \\] Checklist: All unknown parameters only multiply known quantities? X -&gt; \\(\\beta_0*e^{\\beta_1x_i}\\) All parameters are added together? X -&gt; \\(\\beta_0*e^{\\beta_1x_i}\\) All parameters are outside nonlinear functions? X -&gt; \\(e^{\\beta_1x_i}\\) How do we work with nonlinear models? Fit it as it is. Linearization. More about both on day 2! How do we chose a a model? What is your research question? Pattern (phenomenological) vs Process (mechanistic) What do you know about what you are investigating? What do you want from the parameters? Focus: Inference? Prediction? Both? 2. Estimation and Inference: A linear model example Estimation: Getting actual numbers for the model parameters Inference: Making sense of these numbers regarding the population being studied Example: Understanding nitrogen balance in grain legumes Details from Palmero et al. 2024 A simple intercept-slope linear model: \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon \\] in which: \\(y_i\\) represents the proportion (%) of nitrogen (N) derived from the atmosphere (Ndfa) \\(x_i\\) represents the partial N balance (fixed N - removed N by grains) in kg/ha The research question: Which Ndfa (\\(y_i\\)) leads to a neutral N balance (\\(x_i = 0\\))? Code # Data preparation df &lt;- read.csv(&quot;Palmero_data.csv&quot;) df_a &lt;- df[which(df$Scenario == &quot;Scenario A&quot;),] plot(df_a$Ndfa, df_a$PartNbalance, ylab = &quot;Partial N balance (kg/ha)&quot;, xlab = &quot;Ndfa (%)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) I. Loss function approach How does it works? Loss function: How we calculate the distance between observed and predicted by the model. Several different options, the most famous: Least Square - \\(\\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\) This approach produces a line of best fit based on values for \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the distance. No uncertainty around estimation, no p-values, no inference, only parameter values! For our example: Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance (\\(\\theta\\)). \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\] Theta is a derived quantity3 calculated as: \\[ Ndfa = -\\frac{\\beta_0}{\\beta_1} \\] Code # Fit the model m1 &lt;- lm(PartNbalance ~ Ndfa, data = df_a) # Ndfa to achieve neutral N balance b0hat_loss &lt;- as.numeric(coef(m1)[1]) # extract the intercept b1hat_loss &lt;- as.numeric(coef(m1)[2]) # extract the slope thetahat_loss &lt;- -b0hat_loss/b1hat_loss # calculate theta thetahat_loss ## [1] 58.26843 Visual representation of our line of best fit and \\(\\theta\\): Code plot(df_a$Ndfa, df_a$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) abline(m1, col = &quot;red&quot;, lwd = 3) abline(v = thetahat_loss, lwd = 3, lty = 2, col = &quot;green&quot;) II. Likelihood-based approach How does it work? Instead of minimizing the distance, we now maximize the likelihood function. The question is: “If these parameter values were true, how plausible is the data I actually observe?”. Likelihood is about explaining the data you already have4. No more a line of best fit, but an expectation! To define a likelihood, we must introduce assumptions about how the data were generated. We make a distributional assumption for the random error term: \\[ \\varepsilon_i \\sim N(0, \\sigma^2) \\] This distributional assumption is about how the data is generated, it means we believe the deviations between the observed data and the model predictions arise from a normal distribution with mean 0 and variance \\(\\sigma^2\\). Here we have full likelihood based statistical inference (p-values, confidence intervals, etc). For our example: Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance (\\(\\theta\\)). \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\\\ \\varepsilon_i \\sim N(0, \\sigma^2) \\] Code library(nlme) # Fit the model m2 &lt;- gls(PartNbalance ~ Ndfa, data = df_a, method = &quot;ML&quot;) # Ndfa to achieve neutral N balance b0hat_ml &lt;- as.numeric(coef(m2)[1]) # extract the intercept b1hat_ml &lt;- as.numeric(coef(m2)[2]) # extract the slope thetahat_ml &lt;- -b0hat_ml/b1hat_ml # calculate theta thetahat_ml ## [1] 58.26843 Here we can construct confidence intervals by approximating the standard errors using delta method5. Code library(msm) theta_se &lt;- deltamethod(~-x1/x2, mean = coef(m2), cov = vcov(m2)) theta_ci &lt;- c(thetahat_ml-1.96*theta_se, thetahat_ml+1.96*theta_se) theta_ci ## [1] 52.88317 63.65370 Visual representation of our expected values and \\(\\hat{\\theta}\\): Code plot(df_a$Ndfa, df_a$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) abline(m2, col = &quot;red&quot;, lwd = 3) abline(v = thetahat_ml, lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = theta_ci[1], lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = theta_ci[2], lwd = 1, lty = 2, col = &quot;green&quot;) What are we gaining here? More complexity; Uncertainty around estimated \\(\\hat{\\theta}\\); We could also explore uncertainty for the expected value (red line). III. Bayesian approach How does it work? Now, we do not focus on the most likely value of the parameter, we focus on it’s whole distribution, all values it can assume. In this approach, we have additional assumptions concerning our unknown parameters. “Bayesian statistical models provide a useful way to obtain inference and predictions for unobserved quantities [parameters] in nature based on a solid foundation of mathematical rules pertaining to probability.” () Through Bayesian approach, it is very straight forward to account for and explicitly demonstrate uncertainty. We use prior knowledge about the system, to inform our parameters to be estimated. Bayesian estimation does not contradict Maximum Likelihood, it extends it! More on day 3 For our example: Fit a simple intercept-slope linear model to data using least squares and calculate the value of Ndfa that is needed to achieve a neutral N balance (\\(\\theta\\)). \\[ y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i \\\\ \\varepsilon_i \\sim N(0, \\sigma^2)\\] We will re-write this model, using a different notation: \\[ \\mathbf{y} \\sim N(\\boldsymbol{\\beta_0} + \\boldsymbol{\\beta_1}\\mathbf{x}, \\boldsymbol{\\sigma}^2) \\\\ \\boldsymbol{\\beta_0} \\sim N(0, 10^6) \\\\ \\boldsymbol{\\beta_1} \\sim N(0, 10^6) \\\\ \\boldsymbol{\\sigma} \\sim Gamma(2.5, 0.05) \\] This notation helps to clearly identify our assumptions! Code library(rjags) library(coda) set.seed(2026) y &lt;- df_a$PartNbalance x &lt;- df_a$Ndfa # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;N&quot; = nrow(df_a)) initials &lt;- list( list(&quot;b0&quot; = 0.1, &quot;b1&quot; = 0.1, &quot;sigma&quot; = 0.1) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dnorm(mu[i], tau) mu[i] &lt;- b0 + b1*(x[i]) } # Priors b0 ~ dnorm(0, 1/10^6) b1 ~ dnorm(0, 1/10^6) sigma ~ dgamma(2.5, 0.05) # Derived quantity theta &lt;- -b0/b1 tau &lt;- 1/(sigma^2) }&quot; m3 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m3, n.iter = 1000) samples &lt;- coda.samples(m3, variable.names = c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;theta&quot;, &quot;sigma&quot;), n.iter = 1000) #summary(samples) #plot(samples) # Extract draws and calculate expected line draws &lt;- as.matrix(samples) b0_hat &lt;- mean(draws[, &quot;b0&quot;]) b1_hat &lt;- mean(draws[, &quot;b1&quot;]) pred_mean &lt;- b0_hat + b1_hat*x Code par(mfrow = c(2, 2)) hist(draws[, &quot;b0&quot;], freq = FALSE, main = &quot;Posterior dist. b0&quot;, xlab = &quot;b0&quot;) abline(v = mean(draws[, &quot;b0&quot;]), lwd = 2, col = &quot;red&quot;) hist(draws[, &quot;b1&quot;], freq = FALSE, main = &quot;Posterior dist. b1&quot;, xlab = &quot;b1&quot;) abline(v = mean(draws[, &quot;b1&quot;]), lwd = 2, col = &quot;red&quot;) hist(draws[, &quot;theta&quot;], freq = FALSE, main = &quot;Posterior dist. theta&quot;, xlab = &quot;theta&quot;) abline(v = mean(draws[, &quot;theta&quot;]), lwd = 2, col = &quot;red&quot;) par(mfrow = c(1, 1)) Code theta_CrI &lt;- quantile(draws[, &quot;theta&quot;], c(0.025, 0.975)) plot(df_a$Ndfa, df_a$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) lines(df_a$Ndfa, pred_mean, col = &quot;red&quot;, lwd = 2) abline(v = mean(draws[, &quot;theta&quot;]), lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = theta_CrI[1], lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = theta_CrI[2], lwd = 1, lty = 2, col = &quot;green&quot;) What changes? Method b0 b1 theta theta_l theta_u LS -78.38 1.35 58.27 ML -78.38 1.35 58.27 52.88 63.65 Bay -77.68 1.33 58.26 52.08 63.93 Why? Simple model Lot’s of observations - No deficiency on informational content 3. Why use Bayesian? Code df_b &lt;- df[which(df$Scenario == &quot;Scenario B&quot;),] plot(df_b$Ndfa, df_b$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;, ylim = c(-100, 100), xlim = c(0, 110)) abline(a = 0, b = 0, lwd = 3, col = &quot;gold&quot;) You should be concerned about uncertainty here! Let’s try a likelihood approach: Code # Fit the model m4 &lt;- gls(PartNbalance ~ Ndfa, data = df_b, method = &quot;ML&quot;) # Ndfa to achieve neutral N balance b0hat_ml2 &lt;- as.numeric(coef(m4)[1]) # extract the intercept b1hat_ml2 &lt;- as.numeric(coef(m4)[2]) # extract the slope thetahat_ml2 &lt;- -b0hat_ml2/b1hat_ml2 # calculate theta theta_se2 &lt;- deltamethod(~ -x1/x2, mean = coef(m4), cov = vcov(m4)) theta_ci2 &lt;- c(thetahat_ml2-1.96*theta_se2, thetahat_ml2+1.96*theta_se2) plot(df_b$Ndfa, df_b$PartNbalance, xlim = c(0, 110), ylim = c(-100, 100), xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N Balance (kg/ha)&quot;) abline(a = 0, b = 0, lwd = 3, col = &quot;gold&quot;) abline(m4, lwd = 3, col = &quot;red&quot;) abline(v = thetahat_ml2, lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = theta_ci2[1], lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = theta_ci2[2], lwd = 1, lty = 2, col = &quot;green&quot;) How about Bayesian? Code set.seed(2026) y &lt;- df_b$PartNbalance x &lt;- (df_b$Ndfa)/100 # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;N&quot; = nrow(df_b)) initials &lt;- list( list(&quot;theta&quot; = 0.1, &quot;b1&quot; = 0.1, &quot;sigma&quot; = 0.1) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dnorm(mu[i], tau) mu[i] &lt;- -b1*theta + b1*(x[i]) } # Priors b1 ~ dgamma(1.6, 0.8) theta ~ dbeta(62.52, 14.06) sigma ~ dgamma(2.5, 0.05) # Derived quantity b0 &lt;- -b1*theta tau &lt;- 1/(sigma^2) }&quot; m5 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m5, n.iter = 1000) samples2 &lt;- coda.samples(m5, variable.names = c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;theta&quot;, &quot;sigma&quot;), n.iter = 1000) #summary(samples2) #plot(samples2) # Extract draws and calculate expected line draws2 &lt;- as.matrix(samples2) theta_hat2 &lt;- mean(draws2[, &quot;theta&quot;]) b1_hat2 &lt;- mean(draws2[, &quot;b1&quot;]) pred_mean2 &lt;- b1_hat2* + b1_hat2*x Code theta_CrI2 &lt;- quantile(draws2[, &quot;theta&quot;], c(0.025, 0.975)) plot(df_b$Ndfa, df_b$PartNbalance, xlab = &quot;Ndfa (%)&quot;, ylab = &quot;Partial N balance (kg/ha)&quot;, ylim = c(-100, 100), xlim = c(0, 110)) abline(a = 0, b = 0, col = &quot;gold&quot;, lwd = 3) #lines(df_b$Ndfa, pred_mean2, col = &quot;red&quot;, lwd = 2) abline(v = (mean(draws2[, &quot;theta&quot;]))*100, lwd = 3, lty = 2, col = &quot;green&quot;) abline(v = (theta_CrI2[1])*100, lwd = 1, lty = 2, col = &quot;green&quot;) abline(v = (theta_CrI2[2])*100, lwd = 1, lty = 2, col = &quot;green&quot;) Why? Deficient information about \\(\\theta\\) in this dataset. Bayesian offers the opportunity to utilize prior information and a constraint \\(\\hat{\\theta}\\). We treat \\(\\theta\\) as a random variable. Similar assumptions across approaches. 4. Practice for day 1 Sample code is provided for some questions. Code used on the class examples can also be used to work on this practice. Fixation day 1 I. Using the field pea and white lupine data, experiment fitting the simple intercept-slope linear model (\\(y_i = \\beta_0 + \\beta_1*xi + \\varepsilon_i\\)) using the likelihood approach. Code # Data ## Field pea data (Scenario A) fp &lt;- df[which(df$Scenario == &quot;Scenario A&quot;),] ## White lupine data (Scenario B) wl &lt;- df[which(df$Scenario == &quot;Scenario B&quot;),] II. Calculate the derived quantity \\(\\hat{\\theta}\\), and use delta method and bootstrap to obtain confidence intervals. Compare the intervals between the two datasets. Bootstrap will be revisited on day 3, please, have a look at it beforehand! Code # Bootstrap for field pea (fp) library(nlme) # gls function for model fit set.seed(2026) # As we are working with a random (stochastic) process, we need to set seed to be able to reproduce the results ite &lt;- 1000 # How many iterations (repetitions of the process) we will have n &lt;- nrow(fp) # Sample size to be collected (= df size) theta.save &lt;- seq(0, ite-1) # Save theta calculated for each sample # Custom bootstrap algorithm for(i in 1:ite){ sample &lt;- fp[sample(nrow(fp), n, replace = TRUE), ] fit &lt;- gls(PartNbalance ~ Ndfa, data = sample, method = &quot;ML&quot;) theta &lt;- -coef(fit)[1]/coef(fit)[2] theta.save[i] &lt;- theta } # Plot the bootstrap distribution hist(theta.save, freq = FALSE, main = &quot;Bootstrap distribution of theta&quot;, xlab = &quot;theta&quot;) # 95% CI quantile(theta.save, probs = c(0.025, 0.975)) Steps: Sample n elements with replacement from the original data For every sample calculate the desired statistic (here \\(\\theta\\)) Repeat steps 1 and 2 n times (iterations = ite) and save the calculated statistic Plot the calculated statistics which forms the bootstrap distribution Using the bootstrap distribution of desired statistics calculate the 95% CI Pre-day 2 - Nonlinear models I. Fit an logistic model to the data of plant disease progress over time in “nonlin.csv”. Code # Data nl_df &lt;- read.csv(&quot;nonlin.csv&quot;) nl_24 &lt;- nl_df[which(nl_df$year == 2024 &amp; nl_df$var == &quot;a&quot;),] nl_24$p_sev &lt;- nl_24$severity/100 nl_23 &lt;- nl_df[which(nl_df$year == 2023 &amp; nl_df$var == &quot;a&quot;),] nl_23$p_sev &lt;- nl_23$severity/100 I.a. Use linearization to fit a linear model for 2024. We will use a logit transformation: \\[ logit(y_i) = \\beta_0 + \\beta_1*x_i + \\varepsilon_i \\] Where \\(\\beta_1 \\sim r\\) from the nonlinear form. Code # Example for 2024 # For this, we assume K = 1 nl_24$p_sev &lt;- nl_24$p_sev + 0.001 nl_24$l_sev &lt;- log(nl_24$p_sev/(1-nl_24$p_sev)) # Fit the linear model to the logit transformed y m1 &lt;- lm(l_sev ~ time, data = nl_24) # Make predictions logit_pred &lt;- data.frame( time = seq(min(nl_24$time), max(nl_24$time), 1)) logit_pred$pred &lt;- predict(m1, newdata = logit_pred) # Plot predictions plot(nl_24$time, nl_24$l_sev, ylab = &quot;Logit(y)&quot;, xlab = &quot;Days&quot;, ylim = c(-7, 5)) lines(logit_pred$time, logit_pred$pred, lwd = 3, col = &quot;red&quot;) I.b. Use the nls function. First, fit for the year of 2024, then, do it for the year of 2023. Make a plot with predictions. Save models for the next question. \\[ y_i = \\frac{K}{1 + (\\frac{K - y_0}{y_0})e^{-rt_i}} + \\varepsilon_i \\] Code # Example for 2024 # Fit the logistic model m2 &lt;- nls(p_sev ~ K/(1 + ((K-y0)/y0)*exp(-r*time)), data = nl_24, start = list(K = 0.7, y0 = 0.001, r = 0.2)) # Predict using the logistic curve nl_pred &lt;- data.frame( time = seq(min(nl_24$time), max(nl_24$time), 1)) nl_pred$pred &lt;- predict(m2, newdata = nl_pred) # Plot predictions plot(nl_24$time, nl_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(nl_pred$time, nl_pred$pred, lwd = 3, col = &quot;red&quot;) II. Extract model parameters. Check the parameter meaning below. Check the Wald CI. Are values making sense? Disease severity: Percentage of tissue area affected by the disease (in this case, percentage of the leaf area affected) Logistic disease progress curve model parameter meaning: \\(K\\) - Maximum disease potential - Maximum severity level \\(r\\) - Disease rate of growth - How fast it grows \\(y_0\\) - Initial disease level Code summary(m2) # Overall model summary # Example - For K: ## Extract the estimated value K &lt;- as.numeric(coef(m2)[1]) ## Calculate Wald Confidence Interval for the parameter estimate K_sd &lt;- sqrt(vcov(m2)[1]) K_lb &lt;- K-1.96*K_sd # CI Lower K_ub &lt;- K+1.96*K_sd # CI Upper # Notes for you: ## nls in R uses least squares, a loss function approach. Recall: Minimize the loss function. ## To get CI here we are making additional assumptions that we do not make for loss function approach: iid normal residuals. ## This is possible because LS = ML under iid Normal errors with constant variance. ## Why 1.96? It represents the 97.5th percentile of the std N distribution. III. Try fitting a polynomial to this data. Does the predicted curve make sense visually? Code # Example for 2024 # Fit the model m3 &lt;- lm(p_sev ~ poly(time, degree = 3), data = nl_24) # Make predictions poly_pred &lt;- data.frame( time = seq(min(nl_24$time), max(nl_24$time), 1)) poly_pred$pred &lt;- predict(m3, newdata = poly_pred) # Plot predictions plot(nl_24$time, nl_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(poly_pred$time, poly_pred$pred, lwd = 3, col = &quot;red&quot;) About the parameters, are they still carrying biological meaning? How fast is the disease growing? Code summary(m3) We are using scalar notation throughout this class.↩︎ Base function is a known function of the predictor (\\(x\\)) that gets its own coefficient in a linear model. In short, a mathematical transformation of the predictor. For example, the linear model \\(y_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i\\) can be written as \\(y_i = \\beta_0 * 1 + \\beta_1 * x + \\varepsilon_i\\), in which the base functions (\\(b\\)) are \\(b_0(x) = 1\\) and \\(b_1(x) = x\\). For polynomial regression, for example, the base function could be represented as \\(b_d(x_i) = x_i^d\\), with \\(d\\) being the polynomial degree.↩︎ Derived quantities are functions of model parameters↩︎ Finding the parameter values under which the observed data are most plausible according to the assumed data-generating process.↩︎ The Delta Method approximates the uncertainty of a transformed estimator (derived quantity), assuming it was originally approximately normal.↩︎ "],["welcome-to-day-2.html", "Welcome to Day 2!! Nonlinear models", " Welcome to Day 2!! Yesterday we: Discussed what is a linear model and what is not - Linear on the parameters. We showed that linear models \\(\\neq\\) lines. We showed that linear models \\(\\neq\\) simplicity. Discussed different estimation approaches. Loss function approach - Minimize the residual sum (distance between obs and pred.). Likelihood-based approach - Maximize likelihood = Values of parameters that maximize the probability of the observed data under the model we assumed. Bayesian approach - Prior knowledge + data = Values the parameter can assume with different probability - Whole distribution (posterior distribution). Discussed briefly when they are equal (e.g., normal errors) and when they are not. Nonlinear models Outline: Exercises from day 1 Linearization - Pros and cons Mechanistic vs Phenomenological models Identifiability of parameters Working with non-identifiability Fixing weakly identifiable parameters Simplifying the model - Linearization Summary Practice questions What is day 2 about? When parameters matter, inference becomes fragile and every workaround has a cost. 1. Exercises from day 1 You were asked to work with this data: What is it really about? Two years/seasons: 2023 and 2024 Two varieties: a -&gt; Susceptible b -&gt; Moderately resistant Your were asked to fit a linearized form of the logistic model for 2024 variety a Code # Data 2024 variety a a_24 &lt;- df_a[which(df_a$year == 2024),] a_24$p_sev &lt;- a_24$p_sev + 0.001 # Logit transformation a_24$l_sev &lt;- log(a_24$p_sev/(1-a_24$p_sev)) # Logit # Fit the model m1 &lt;- lm(l_sev ~ time, data = a_24) # Make predictions m1_pred &lt;- data.frame( time = seq(min(a_24$time), max(a_24$time), by = 1) ) m1_pred$pred &lt;- predict(m1, newdata = m1_pred) # Plot predictions m1_pred$inv_logit &lt;- 1/(1+exp(-m1_pred$pred)) plot(a_24$time, a_24$l_sev, ylab = &quot;Logit(y)&quot;, xlab = &quot;Days&quot;, ylim = c(-7, 5)) lines(m1_pred$time, m1_pred$pred, lwd = 3, col = &quot;red&quot;) Code plot(a_24$time, a_24$p_sev, ylab = &quot;y_prop.&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1.1)) lines(m1_pred$time, m1_pred$inv_logit, lwd = 3, col = &quot;gold&quot;) What is actually going on here? Logit: \\[ logit(y) = log(\\frac{y}{K-y}) \\] We have to assume a value for \\(K\\), “losing” this parameter. Our intercept (\\(\\beta_0\\)) it not as simple as before: \\[ \\beta_0 = log(\\frac{y_0}{K-y_0}) \\] We also “lose” \\(y_0\\). We still have our slope \\(\\beta_1\\), and we can use it to make inference about the disease growth, but it is not the same as \\(r\\). \\(\\beta_1\\): Multiply \\(x_i\\) and is additive, each unit change in \\(\\mathbf{x}\\) adds \\(\\beta_1\\) to \\(\\mathbf{y}\\). Shifts the mean linearly. Effect is independent on \\(\\mathbf{y}\\) value. No bounds. No direct biological meaning. Easier to estimate with low correlation to \\(\\beta_0\\). \\(r\\) Multiply \\(x_i\\), but is on an exponential, each unit change in \\(\\mathbf{x}\\) is multiplicative in the growth (differential equation6). Affects shape and trajectory of the entire curve. Effect changes with \\(\\mathbf{y}\\) value. Bounded between 0 and \\(K\\) Direct biological meaning. Estimation sensitive to information on the data, often correlated with \\(K\\). The actual linearized model: \\[ log(\\frac{y_i}{1 - y_i}) = log(\\frac{y_0}{1 - y_0}) + \\beta_1*x_i + \\varepsilon_i \\] Pros: Simple to use; require less information from the data collected to fit. Cons: No inference on important parameters such as \\(K\\) and \\(y_0\\); more complex to make sense of the response variable \\(logit(severity)\\); sensibility to extreme values (close to 0 or 100); \\(\\beta_1\\) and \\(r\\) do not have the same meaning (\\(\\beta_1*x_i\\) adds, while \\(r*x_i\\) is exponential). Also, more assumptions = More bias! To be discussed on day 3! You were asked to fit two different models to this data, a logistic and a polynomial for both 2023 and 2024. Mechanistic vs Phenomenological models - First, we will focus on these two models in 2024 for variety a: Logistic model Code # Fit the model m2 &lt;- nls(p_sev ~ K/(1 + ((K-y0)/y0)*exp(-r*time)), data = a_24, start = list(K = 0.7, y0 = 0.001, r = 0.2)) # Make predictions m2_pred &lt;- data.frame( time = seq(min(a_24$time), max(a_24$time), by = 1)) m2_pred$pred &lt;- predict(m2, newdata = m2_pred) # Plot predictions plot(a_24$time, a_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(m2_pred$time, m2_pred$pred, lwd = 3, col = &quot;red&quot;) Polynomial Code # Fit the model m3 &lt;- lm(p_sev ~ poly(time, degree = 3), data = a_24) # Make predictions m3_pred &lt;- data.frame( time = seq(min(a_24$time), max(a_24$time), by = 1)) m3_pred$pred &lt;- predict(m3, newdata = m3_pred) # Plot predictions plot(a_24$time, a_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(m3_pred$time, m3_pred$pred, lwd = 3, col = &quot;red&quot;) What is the difference visually? Why would we choose one or the other? Board What is the disease rate of growth? Code round(coef(m2)[3], 4) ## r ## 0.3339 Identifiability of parameters - Now, we will focus on the logistic model for variety a in 2023 and 2024: Here, we will add the following assumption: \\[ \\varepsilon_i \\sim i.i.d. N(0, \\sigma^2) \\] This way we can get Wald confidence intervals7. What is identifiability? In general terms, when a parameter is weakly constrained ~ We are very uncertain about that parameter Two types: Structural: This is caused by the model structure Practical: Caused by the data - Limited data lack information on a specific parameter Board What it is - Example Why is it very important in nonlinear models To understand what is causes, look at the parameters of our model for both years: High uncertainty around parameter estimates! - Classical symptom of weak identifiability Another classical symptom is correlation between parameters. How does it look like for our models? 2023 correlation matrix ## K y0 r ## K 1.0000000 0.9761482 -0.9771736 ## y0 0.9761482 1.0000000 -0.9999097 ## r -0.9771736 -0.9999097 1.0000000 2024 correlation matrix ## K y0 r ## K 1.0000000 0.6486111 -0.6869020 ## y0 0.6486111 1.0000000 -0.9946642 ## r -0.6869020 -0.9946642 1.0000000 Why it happens? The data inform combinations of parameters instead of individual parameters It created dependencies - Dependencies between r and K. 2. Working with weakly identifiable parameters What are our options? Adjust the experimental design and/or data collection method Collect more data associated with the weakly identified parameters Reparametrize weakly identified parameters Simplify the model (e.g. fixing weakly identified parameters) Impose constrains on these parameters using prior knowledge A good paper about identifiability: Preston et al. 2025 Now, we will focus on variety a in 2023: Recall 2023 disease progress in time: Fixing weakly identifiable parameters If our biggest interest concerns other parameters, we can fix weakly identified parameters. In this case, \\(r\\) is usually the parameter of greatest interest, once it tells how fast the epidemic progresses. Let’s assume \\(K = 1\\): \\[ y_i = \\frac{1}{1+(\\frac{1 - y_0}{y_0})*e^{-rx_i + \\varepsilon_i}} \\] Code # Data variety a, 2023 a_23 &lt;- df[which(df$var == &quot;a&quot; &amp; df$year == 2023),] # Fit the model m4 &lt;- nls(p_sev ~ 100/(1 + ((100-y0)/y0)*exp(-r*time)), data = a_23, start = list(y0 = 0.001, r = 0.2)) # Make predictions m4_pred &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) m4_pred$pred &lt;- predict(m4, newdata = m4_pred) # Plot predictions plot(a_23$time, a_23$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1)) lines(m4_pred$time, m4_pred$pred, lwd = 3, col = &#39;red&#39;) How it compares to estimating \\(K\\)? How about the parameters? The data does not inform \\(K\\) when we fix it, we inform it! Simplifying the model - Linearization Recall: Here we have to assume a value for \\(K\\) in order to use the logic transformation, therefore \\(K = 1\\). Our response variable becomes logit(severity). Code # Trasnform the data with the logit transformation a_23$l_sev &lt;- a_23$p_sev/(1-a_23$p_sev) # Fit the linear model m6 &lt;- lm(l_sev ~ time, data = a_23) # Make predictions m6_pred &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time), by = 1) ) m6_pred$pred &lt;- predict(m6, newdata = m6_pred) # Plot predictions plot(a_23$time, a_23$l_sev, ylab = &quot;Logit(y)&quot;, xlab = &quot;Days&quot;, ylim = c(-1, 3)) lines(m6_pred$time, m6_pred$pred, lwd = 3, col = &quot;red&quot;) And extract or slope \\(\\beta_1\\) and check uncertainty: Code b1 &lt;- as.numeric(coef(m6)[2]) se6 &lt;- sqrt(diag(vcov(m6))) lb_b1 &lt;- as.numeric(b1 - 1.96*se6[&quot;time&quot;]) ub_b1 &lt;- as.numeric(b1 + 1.96*se6[&quot;time&quot;]) print(paste(&quot;Estimate:&quot;, round(b1, 4), &quot;Lower:&quot;, round(lb_b1, 4), &quot;Upper:&quot;, round(ub_b1, 4))) ## [1] &quot;Estimate: 0.0855 Lower: 0.0451 Upper: 0.126&quot; Contrast \\(r\\) and \\(\\beta_1\\): Parameter Estimate L.CI U.CI b1 0.085519 0.045051 0.1259871 r 0.336100 -1.098000 1.7702000 \\(\\beta_1\\) is usually well identified with little data, in contrast to \\(r\\) These parameters represent different things, as discussed above \\(\\beta_1\\) describes the change in \\(\\mathbf{y}\\) with one unit change in \\(\\mathbf{x}\\) \\(r\\) describes how quickly the epidemic accelerates and approaches its upper bound 3. Summary Today we discussed: Linearization Mechanistic vs Phenomenological models Identifiability What it is How to work with it - Fixing parameters and simplification Which models did we use? Logistic Disease Progress Curve (nonlinear, mechanistic): \\[ y_i = \\frac{K}{1 + (\\frac{K - y_0}{y_0})e^{-r*x_i + \\varepsilon_i}} \\] Logit-transformed linear model (linear, phenomenological): \\[ log(\\frac{y_i}{1 - y_i}) = log(\\frac{y_0}{1 - y_0}) + \\beta_1*x_i + \\varepsilon_i \\] Which assumption we made? For all models: \\[ \\varepsilon_i \\sim i.i.d. N(0, \\sigma^2) \\] For the linearized: \\[ K = 1 \\\\[10pt] \\varepsilon_i \\sim i.i.d. N(0, \\sigma^2) \\] *nls function in R uses Least Squares, a loss function approach, but under the assumption that errors are independent and individually distributed, Likelihood approach = Loss function approach. 4. Practice questions Sample code is provided for some questions. Code used on the class examples can also be used to work on this practice. Code # Data df &lt;- read.csv(&quot;nonlin.csv&quot;) df$p_sev &lt;- df$severity/100 I. For 2023, fit the nonlinear logistic model to variety a and b. Extract \\(K\\) and \\(r\\) and discuss the effect of genetic resistance in these parameters. Code # Let&#39;s fit the model for each variety individually # Data a_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),] b_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;b&quot;),] # Fit the models ma &lt;- nls(p_sev ~ K/(1+((K-y0)/y0)*exp(-r*time)), data = a_23, start = c(K = 0.7, y0 = 0.0001, r = 0.2)) mb &lt;- nls(p_sev ~ K/(1+((K-y0)/y0)*exp(-r*time)), data = b_23, start = c(K = 0.7, y0 = 0.0001, r = 0.2)) # Make predictions pred_df &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) pred_df$pred_a &lt;- predict(ma, newdata = pred_df) pred_df$pred_b &lt;- predict(mb, newdata = pred_df) # Plot predictions plot(a_23$time, a_23$p_sev, type = &quot;p&quot;, col = &quot;darkred&quot;, ylim = c(0, 1), ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;) points(b_23$time, b_23$p_sev, col = &quot;orange&quot;) lines(pred_df$time, pred_df$pred_a, lwd = 3, col = &#39;red&#39;) lines(pred_df$time, pred_df$pred_b, lwd = 3, col = &#39;gold&#39;) # Extract parameters ## K K_a &lt;- coef(ma)[1] K_a K_b &lt;- coef(mb)[1] K_b ## r r_a &lt;- coef(ma)[3] r_a r_b &lt;- coef(mb)[3] r_b I.a. Write down the model you are using in I and which assumption you are making (fixation). II. Repeat what you did in I, but assume \\(K = 1\\). Can you see differences? Code # Let&#39;s fit the model for each variety individually # Data a_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),] b_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;b&quot;),] # Fit the models ma &lt;- nls(p_sev ~ 1/(1+((1-y0)/y0)*exp(-r*time)), data = a_23, start = c(y0 = 0.0001, r = 0.2)) mb &lt;- nls(p_sev ~ 1/(1+((1-y0)/y0)*exp(-r*time)), data = b_23, start = c(y0 = 0.0001, r = 0.2)) # Make predictions pred_df &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) pred_df$pred_a &lt;- predict(ma, newdata = pred_df) pred_df$pred_b &lt;- predict(mb, newdata = pred_df) # Plot predictions plot(a_23$time, a_23$p_sev, type = &quot;p&quot;, col = &quot;darkred&quot;, ylim = c(0, 1), ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;) points(b_23$time, b_23$p_sev, col = &quot;orange&quot;) lines(pred_df$time, pred_df$pred_a, lwd = 3, col = &#39;red&#39;) lines(pred_df$time, pred_df$pred_b, lwd = 3, col = &#39;gold&#39;) # Extract parameters ## r r_a &lt;- coef(ma)[2] r_a r_b &lt;- coef(mb)[2] r_b III. Repeat what you did in I and II, but now, use the logit transformation on the data and a linear model. Compare the disease progress in varieties a and b. Code # Data a_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),] a_23$l_sev &lt;- a_23$p_sev/(1-a_23$p_sev) b_23 &lt;- df[which(df$year == 2023 &amp; df$var == &quot;b&quot;),] b_23$l_sev &lt;- b_23$p_sev/(1-b_23$p_sev) # Fit the models ma &lt;- lm(l_sev ~ time, data = a_23) mb &lt;- lm(l_sev ~ time, data = b_23) # Make predictions pred_df &lt;- data.frame( time = seq(min(a_23$time), max(a_23$time)) ) pred_df$pred_a &lt;- predict(ma, newdata = pred_df) pred_df$pred_b &lt;- predict(mb, newdata = pred_df) # Plot predictions plot(a_23$time, a_23$l_sev, type = &quot;p&quot;, col = &quot;darkred&quot;, ylim = c(-0.3, 2), ylab = &quot;Logit(y)&quot;, xlab = &quot;Days&quot;) points(b_23$time, b_23$l_sev, col = &quot;orange&quot;) lines(pred_df$time, pred_df$pred_a, lwd = 3, col = &#39;red&#39;) lines(pred_df$time, pred_df$pred_b, lwd = 3, col = &#39;gold&#39;) # Extract parameters ## b1 b1_a &lt;- coef(ma)[2] b1_a b1_b &lt;- coef(mb)[2] b1_b III.a. Write down the model you are using in I and which assumption you are making (fixation). VI. Repeat question I for 2024. \\(\\frac{dy}{dt} = \\mathbf{ry}(1-\\frac{\\mathbf{y}}{\\mathbf{K}})\\)↩︎ Wald CI measure local uncertainty (around the parameter estimate) and not at the whole parameter space (all values it could assume - global), therefore, they might be more optimistic than global CI. Bootstrap or Bayesian approach would provide more closely a global uncertainty measure once they explore a broader region of the parameter space.↩︎ "],["welcome-to-day-3.html", "Welcome to day 3!!! Bayesian Nonlinear models", " Welcome to day 3!!! What have we learned so far? Day 1 What is and what isn’t a linear model. Estimation methods: Loss function; Likelihood-based; Bayesian. When they are similar (e.g. normal errors). Day 2 Linearization -&gt; From a curve, to a line… to a curve. Mechanistic and Phenomenological models -&gt; Describe what causes patterns vs Describe the patterns. Practical identifiability of parameters -&gt; When our data cannot inform our model parameters. How to work with weak identifiability: Fixing parameters. Simplifying the model. Bayesian Nonlinear models Outline: Exercises from day 2 Questions I and IV - Model selection, why? Bayesian nonlinear models Bayesian logistic disease progress curve Why use Bayesian? Wrap-up What is day 3 about? Imposing constraint on parameters using prior knowledge is done using Bayesian statistics. In Bayesian models, this is done through probabilistic assumptions on parameters, making uncertainty and modeling decisions explicit, while supporting biologically driven decisions. 1. Exercises from day 2 Questions I and IV - Model selection, why? I. For 2023, fit the nonlinear logistic model to variety a and b. Extract \\(K\\) and \\(r\\) and discuss the effect of genetic resistance in these parameters. Variety K r a 0.6684032 0.3361402 b 0.5085910 0.3623941 VI. Repeat question I for 2024. How did it go? More specifically, where you able to fit it for variety b? What we have: What we expect: The blue line represents an exponential model: \\[ y_i = y_0*e^{r_e*x_i} \\] Why don’t we use this model instead? I have been pushing the logistic model for this data, even if it might not be the one that “best fit” it. Why? Discussion 2. Bayesian nonlinear models Data alone was insufficient, we need more assumptions! A parallel with day 2: When data was insufficient, we made assumptions: \\(K = 1\\) + Nonlinear logistic model. \\(K = 1\\) + Linearization. Assume the nonlinear relationship is well approximated by a linear one. Assume inference on linearized model parameters is equivalent to the nonlinear. Mechanistic meaning of the nonlinear model is preserved. *Bayesian makes assumptions explicit vs previous approaches. When using a Bayesian approach, we will be making assumptions, but not as deterministic as \\(K = 1\\), or changing the model structure. We will treat all parameters as random variables. What does it mean? When we write the model: \\[ y_i = \\beta_0 + \\beta_1*x_i + \\varepsilon_i \\\\[10pt] \\varepsilon_i \\sim N(0, \\sigma^2) \\] Is equivalent to write: \\[ y_i \\sim N(\\mu_i, \\sigma^2) \\\\[10pt] \\mu_i = \\beta_0 + \\beta_1*x_i \\] And this reads: \\(y_i\\) arises from a normal distribution with expectation controlled by the mathematical model and variance \\(\\sigma^2\\). What is expectation? It is where the distribution centers, the mean. \\(\\mathbf{y}\\) is a random variable8 that arises from a distribution. Now, what have we been doing so far with the logistic model? \\[ y_i \\sim N(\\mu_i, \\sigma^2) \\\\[10pt] \\mu_i = \\frac{K}{1+(\\frac{K-y_0}{y_0})*e^{-r*x_i}} \\] This is not Bayesian. Recall, under this assumption: LS = ML This is not working because of weak identifiability. But we want this logistic equation, let’s make more assumptions! Bayesian logistic disease progress curve The first assumption we can make regards our response variable. Is our model matching the data generating process? Severity = Proportion -&gt; Bounded 0 - 1 Beta distribution (Beta regression - Ferrari &amp; Cribari-Neto, 2010). \\[ y_i \\sim Beta(\\mu_i, \\phi) \\\\[10pt] \\mu_i = \\frac{K}{1+(\\frac{K-y_0}{y_0})*e^{-r*x_i}} \\] This still not Bayesian. We are just choosing a more appropriate distribution for our response variable. We can do that using likelihood-based estimation. Now, let’s get Bayesian! We will treat our unknown quantities (parameters) as random variables. How does it work? Recall the assumption: \\(K = 1\\) It is the same as: \\(K \\sim N(1, 0)\\) This is the strongest and most restrictive assumption! If we “relax” this assumption, and let \\(K\\) vary a little more, let’s say \\(K \\sim N(1, 5)\\), this what happens: Now, do you think these values make sense? \\(K\\) has a biological meaning - Maximum disease potential. What if we impose a constraint on this parameter? \\[ K \\sim Beta(6, 2) \\] How we are choosing that? Beta distribution - Bounds. Leaning towards higher values - \\(K\\) represents the maximum disease potential. Let it vary from very small maximum to the maximum possible (1). This is what we call prior distribution, we use prior knowledge to inform unknown quantities in the model. “Before we see the data, what values we expect this parameter to assume?” Priors are constructed using: Expert knowledge. Published information. How does it look like for our remaining parameters? \\(y_0\\) ~ \\(Unif(0.001, 0.1)\\) \\(r\\) ~ \\(Unif(0.1, 0.5)\\) \\(\\phi\\) ~ \\(Gamma(24, 2)\\) Dispersion parameter ~ Inverse of the variance We end up with this Bayesian model: \\[ y_i \\sim Beta(\\mu_i, \\phi) \\\\[10pt] \\mu_i = \\frac{K}{1 + (\\frac{K - y_0}{y_0})*e^{-r*x_i}} \\\\[20pt] K \\sim Beta(6, 2) \\\\[10pt] y_0 \\sim Unif(0.001, 0.1) \\\\[10pt] r \\sim Unif(0.1, 0.5) \\\\[10pt] \\phi \\sim Gamma(24, 2) \\] Let’s fit the model for the variety b in 2024: Code library(rjags) library(coda) set.seed(2026) y &lt;- df[which(df$year == 2024 &amp; df$var == &quot;b&quot;),]$p_sev + 0.001 x &lt;- df[which(df$year == 2024 &amp; df$var == &quot;b&quot;),]$time N &lt;- nrow(df[which(df$year == 2024 &amp; df$var == &quot;b&quot;),]) x.pred &lt;- seq(1, max(x)+20, by = 1) N_pred &lt;- length(x.pred) # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;x.pred&quot; = x.pred, &quot;N&quot; = N, &quot;N_pred&quot; = N_pred) initials &lt;- list( list(&quot;K&quot; = 0.7, &quot;r&quot; = 0.2, &quot;y0&quot; = 0.001, &quot;phi&quot; = 10) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dbeta(alpha[i], beta[i]) alpha[i] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x[i])))*phi beta[i] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x[i]))))*phi } # Posterior pred. for(z in 1:N_pred){ y_pred[z] ~ dbeta(alpha_pred[z], beta_pred[z]) alpha_pred[z] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z])))*phi beta_pred[z] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z]))))*phi } # Priors y0 ~ dunif(0.001, 0.1) K ~ dbeta(6, 2) r ~ dunif(0.1, 0.5) phi ~ dgamma(24, 2) }&quot; m1 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m1, n.iter = 10000) samples &lt;- coda.samples(m1, variable.names = c(&quot;K&quot;, &quot;y0&quot;, &quot;r&quot;, &quot;phi&quot;, &quot;y_pred&quot;), n.iter = 10000) # Extract draws ## Parameters par_post &lt;- as.matrix(samples[,c(1, 2, 3, 4)]) ## Predictions pred_post &lt;- as.matrix(samples[,c(5:61)]) How the predictions (expected value) looks like with 95% Posterior Predictive Interval9: Code # Quantiles for 95% Credible Interval quant &lt;- apply(pred_post, 2, quantile, probs = c(0.025, 0.975)) # DF for prediction bay_pred &lt;- data.frame( y.pred = colMeans(pred_post), y.lb = quant[1,], y.ub = quant[2,], x.pred = x.pred ) # Plot prediction plot(bay_pred$x.pred, bay_pred$y.pred, type = &quot;l&quot;, ylab = &quot;Pred. severity (mean)&quot;, xlab = &quot;Days&quot;, ylim = c(-0.1, 1.1), lwd = 3, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.lb, lty = 2, lwd = 3, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.ub, lty = 2, lwd = 3, col = &#39;red&#39;) abline(h = 1, lty = 3, col = &#39;black&#39;, lwd = 2) abline(h = 0, lty = 3, col = &#39;black&#39;, lwd = 2) How about the parameters? Code par(mfrow = c(2, 2)) # For K hist(par_post[,1], ylab = &quot;[K]&quot;, xlab = &quot;K&quot;, main = &quot;Post. dist. K&quot;, freq = FALSE) abline(v = mean(par_post[,1]), lty = 2, lwd = 3, col = &#39;red&#39;) # For r hist(par_post[,3], ylab = &quot;[r]&quot;, xlab = &quot;r&quot;, main = &quot;Post. dist. r&quot;, freq = FALSE) abline(v = mean(par_post[,3]), lty = 2, lwd = 3, col = &#39;red&#39;) # For y0 hist(par_post[,4], ylab = &quot;[y0]&quot;, xlab = &quot;y0&quot;, , main = &quot;Post. dist. y0&quot;, freq = FALSE) abline(v = mean(par_post[,4]), lty = 2, lwd = 3, col = &#39;red&#39;) par(mfrow = c(1, 1)) How posterior compare to prior? Let’s look at \\(r\\). Recall: \\(r \\sim Unif(0.1, 0.5)\\) Code # Post. dist. r hist(runif(1000,0.1, 0.5), freq = FALSE, main = &quot;Prior dist. r&quot;, ylab = &quot;[r]&quot;, xlab = &quot;r&quot;, xlim = c(0.1, 0.5)) Code # Pior dist. r hist(par_post[,3], ylab = &quot;[r]&quot;, xlab = &quot;r&quot;, main = &quot;Posterior dist. r&quot;, freq = FALSE, xlim = c(0.1, 0.5)) Why use Bayesian? Let’s use variety a in 2023. Code #### Fit the Bayesian model for 2024 var a #### set.seed(2026) library(rjags) y &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),]$p_sev + 0.001 x &lt;- df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),]$time N &lt;- nrow(df[which(df$year == 2023 &amp; df$var == &quot;a&quot;),]) x.pred &lt;- seq(1, max(x)+20, by = 1) N_pred &lt;- length(x.pred) # JAGS code data_line &lt;- list(&quot;y&quot; = y, &quot;x&quot; = x, &quot;x.pred&quot; = x.pred, &quot;N&quot; = N, &quot;N_pred&quot; = N_pred) initials &lt;- list( list(&quot;K&quot; = 0.7, &quot;r&quot; = 0.2, &quot;y0&quot; = 0.001, &quot;phi&quot; = 10) ) model_string &lt;- &quot;model { # Likelihood for(i in 1:N){ y[i] ~ dbeta(alpha[i], beta[i]) alpha[i] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x[i])))*phi beta[i] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x[i]))))*phi } # Posterior pred. for(z in 1:N_pred){ y_pred[z] ~ dbeta(alpha_pred[z], beta_pred[z]) alpha_pred[z] &lt;- (K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z])))*phi beta_pred[z] &lt;- (1-(K/(1 + ((K-y0)/y0)*exp(-r*x.pred[z]))))*phi } # Priors y0 ~ dunif(0.001, 0.1) K ~ dbeta(6, 2) r ~ dunif(0.1, 0.5) phi ~ dgamma(24, 2) }&quot; m2 &lt;- jags.model(textConnection(model_string), inits = initials, data = data_line, n.chains = 1) update(m2, n.iter = 10000) samples &lt;- coda.samples(m2, variable.names = c(&quot;K&quot;, &quot;y0&quot;, &quot;r&quot;, &quot;phi&quot;, &quot;y_pred&quot;), n.iter = 10000) # Extract draws ## Parameters par_post &lt;- as.matrix(samples[,c(1, 2, 3, 4)]) ## Predictions pred_post &lt;- as.matrix(samples[, c(5:77)]) Code #### Fit the model using likelihood to 2023 var a #### a_24 &lt;- df[which(df$var == &quot;a&quot; &amp; df$year == 2023),] # Fit the model m3 &lt;- nls(p_sev ~ K/(1+((K-y0)/y0)*exp(-r*time)), start = c(K = 0.7, r = 0.2, y0 = 0.001), data = a_24) # Predictions with prediction interval df_pred &lt;- data.frame( time = seq(1, max(a_24$time)+20, by = 1) ) library(mvtnorm) # Package for multivariate normal set.seed(2026) ite &lt;- 10000 # Number of iterations/simulations par_hat &lt;- coef(m3) # Extract parameter estimated var_cov &lt;- vcov(m3) # Extract var covar matrix sigma_hat &lt;- summary(m3)$sigma # Extract standard deviation par_sim &lt;- rmvnorm(ite, par_hat, var_cov) # Simulate parameters - Draws from a multivariate normal using our expected value for the parameter and their variance time &lt;- df_pred$time sim.save &lt;- matrix(nrow = ite, ncol = length(time)) for(i in 1:ite){ K &lt;- par_sim[i,&quot;K&quot;] y0 &lt;- par_sim[i, &quot;y0&quot;] r &lt;- par_sim[i, &quot;r&quot;] mu &lt;- K/(1+((K-y0)/y0)*exp(-r*time)) y_sim &lt;- rnorm(length(time), mu, sigma_hat) sim.save[i,] &lt;- y_sim } # Prediction + pred intervals on the df df_pred$pred &lt;- predict(m3, newdata = df_pred) df_pred$pred_ub &lt;- apply(sim.save, 2, quantile, 0.975) df_pred$pred_lb &lt;- apply(sim.save, 2, quantile, 0.025) Code # Plot predictions with interval for iid normal assumption plot(a_24$time, a_24$p_sev, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, main = &quot;Likelihood approach with minimal assumptions&quot;, ylim = c(-0.5, 1.1)) lines(df_pred$time, df_pred$pred, lwd = 3, col = &#39;red&#39;) lines(df_pred$time, df_pred$pred_ub, lty = 2, col = &#39;red&#39;) lines(df_pred$time, df_pred$pred_lb, lty = 2, col = &#39;red&#39;) Code # Plot predictions with interval for Bayesian model # Quantiles for 95% Credible Interval quant &lt;- apply(pred_post, 2, quantile, probs = c(0.025, 0.975)) # DF for prediction bay_pred &lt;- data.frame( y.pred = colMeans(pred_post), y.lb = quant[1,], y.ub = quant[2,], x.pred = x.pred ) # Plot prediction plot(x, y, ylab = &quot;Severity (prop.)&quot;, xlab = &quot;Days&quot;, main = &quot;Bayesian model&quot;, ylim = c(-0.1, 1.1)) lines(bay_pred$x.pred, bay_pred$y.pred, lwd = 3, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.lb, lty = 2, col = &#39;red&#39;) lines(bay_pred$x.pred, bay_pred$y.ub, lty = 2, col = &#39;red&#39;) Code # Likelihood uncertainty width lt1 &lt;- df_pred$pred_ub[1] - df_pred$pred_lb[1] lt2 &lt;- df_pred$pred_ub[37] - df_pred$pred_lb[37] lt3 &lt;- df_pred$pred_ub[73] - df_pred$pred_lb[73] # Bayesian uncertainty width bt1 &lt;- bay_pred$y.ub[1] - bay_pred$y.lb[1] bt2 &lt;- bay_pred$y.ub[37] - bay_pred$y.lb[37] bt3 &lt;- bay_pred$y.ub[73] - bay_pred$y.lb[73] # Table with distance library(knitr) library(kableExtra) table_data &lt;- data.frame( Estimation = c(&quot;Min. Assump.&quot;, &quot;Bayesian&quot;), Day_1_Width = c(round(lt1, 4), round(bt1, 4)), Relative_uncertainty = c(&quot; &quot;, round((bt1/lt1)*100, 2)), Day_37_Width = c(round(lt2, 4), round(bt2, 4)), Relative_uncertainty = c(&quot; &quot;, round((bt2/lt2)*100, 2)), Day_73_Width = c(round(lt3, 4), round(bt3, 4)), Relative_uncertainty = c(&quot; &quot;, round((bt3/lt3)*100, 2)) ) knitr::kable(table_data, align = &quot;c&quot;) %&gt;% kable_styling(full_width = TRUE, position = &quot;center&quot;) Estimation Day_1_Width Relative_uncertainty Day_37_Width Relative_uncertainty.1 Day_73_Width Relative_uncertainty.2 Min. Assump. 0.3903 0.9748 0.9557 Bayesian 0.0202 5.18 0.4173 42.81 0.5491 57.46 How about the parameters? Estimation Parameter Estimate Est_lb Est_ub Min. Assump. K 0.6684032 0.0534314 1.2833750 Min. Assump. r 0.3361402 -1.0979691 1.7702495 Min. Assump. y0 0.0000002 -0.0000130 0.0000134 Bayesian K 0.8349453 0.6201002 0.9756110 Bayesian r 0.1245953 0.1026214 0.1438062 Bayesian y0 0.0019489 0.0010201 0.0044708 3. Wrap-up There is no right or wrong! There are different assumptions, and each carry advantages and disadvantages. Choosing assumptions should be done consciously, keeping in mind that they might change the analysis results, and how you make inference. Bayesian is not a better approach. It is another tool in your toolbox, and might be helpful in some situations. Before: Less uncertainty = More data Now: Other options -&gt; Less uncertainty = More assumptions All workshop material is available on this page. Feel free to contact us as well if you have questions! Thank you very much for you attention and for attending this workshop, this was a great opportunity for us all to learn more together! Stay tuned for the next workshop “Bayesian Modeling of Designed Experiments” on March 2, 4, and 5. By definition, a random variable assigns a numerical value to the outcome of a random experiment.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
